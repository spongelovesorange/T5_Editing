# File: evaluate_datasets.py (Corrected Version)

#!/usr/bin/env python3
"""
P5æ¨èæ¨¡å‹æ•°æ®é›†åˆ’åˆ†è¯„ä¼°
æŒ‰ç…§ä¿ç•™é›†å’Œé—å¿˜é›†åˆ†åˆ«è¯„ä¼°æŒ‡å®šæ¨¡å‹æ€§èƒ½

ç”¨æ³•:
    python evaluate_datasets.py --model_path models/ML1M_sequential.pt
    python evaluate_datasets.py --model_path models/ML1M_sequential_unlearned.pt --output_suffix _after_unlearning
    python evaluate_datasets.py --original_model models/original.pt --unlearned_model models/unlearned.pt --compare

å‚æ•°è¯´æ˜:
    --model_path          å•ä¸ªæ¨¡å‹è·¯å¾„
    --original_model      åŸå§‹æ¨¡å‹è·¯å¾„ï¼ˆå¯¹æ¯”æ¨¡å¼ï¼‰
    --unlearned_model     é—å¿˜åæ¨¡å‹è·¯å¾„ï¼ˆå¯¹æ¯”æ¨¡å¼ï¼‰
    --compare             å¯ç”¨å¯¹æ¯”æ¨¡å¼ï¼ŒåŒæ—¶è¯„ä¼°åŸå§‹å’Œé—å¿˜åæ¨¡å‹
    --output_suffix       è¾“å‡ºæ–‡ä»¶åç¼€
    --forget_ratio        é—å¿˜é›†æ¯”ä¾‹ (é»˜è®¤: 0.05)
    --eval_sample_size    è¯„ä¼°æ ·æœ¬å¤§å° (é»˜è®¤: 50)
    --k_values            è¯„ä¼°çš„Kå€¼åˆ—è¡¨ (é»˜è®¤: 10,20)
    --save_predictions    ä¿å­˜æ¨èç»“æœ
    --verbose             è¯¦ç»†è¾“å‡º
"""

import os
import sys
import time
import json
import random
import re
import logging
import argparse
from types import SimpleNamespace
from collections import defaultdict
from typing import Optional, Any, Dict

import torch
import torch.nn as nn
import math

from transformers import T5ForConditionalGeneration, T5Tokenizer, T5Config

# optional progress bar
try:
    from tqdm import tqdm
except Exception:
    tqdm = None

# Local imports
from src.model_wrapper import P5ModelWrapper
from src.dual_memory import load_dual_memory_runtime
from src.p5_evaluator import P5RecommendationEvaluator

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

PROJECT_ROOT = os.path.abspath(os.path.dirname(__file__))


# Fallback stub for WISEUnlearningEditor when real implementation isn't imported here.
# The real implementation lives in src.dual_memory or other modules and will be used
# when initialize_model loads dual artifacts. This stub preserves expected attributes
# to allow static analysis and partial runtime flows that don't exercise editor behavior.
class WISEUnlearningEditor:
    def __init__(self, model, tokenizer=None, config=None):
        self.model = model
        self.tokenizer = tokenizer
        self.config = config or {}
        self.side_modules = {}
        self.router_classifier = None
        self.epsilon_threshold = 0.0
        self.router_feature_dataset = None
        self.router_input_dim = 1
        self.router_feature_mode = 'delta_l2'
        self.router_prob_threshold = None
        self.rescan_before_eval = False


class DatasetEvaluator:
    def __init__(self, args: argparse.Namespace):
        # Save original args
        self.args = args
        self.compare_mode = bool(getattr(args, 'compare', False))
        self.model_path = getattr(args, 'model_path', None)
        self.original_model_path = getattr(args, 'original_model', None)
        self.unlearned_model_path = getattr(args, 'unlearned_model', None)
        self.output_suffix = getattr(args, 'output_suffix', '') or ''
        self.forget_ratio = getattr(args, 'forget_ratio', 0.01)
        self.eval_sample_size = getattr(args, 'eval_sample_size', 50)
        k_vals = getattr(args, 'k_values', '10,20')
        if isinstance(k_vals, str):
            self.k_values = [int(x) for x in k_vals.split(',') if x.strip()]
        else:
            self.k_values = list(map(int, k_vals))
        self.eval_rescan_mode = getattr(args, 'eval_rescan_mode', 'auto')
        self.save_predictions = getattr(args, 'save_predictions', False)
        self.skip_all_users = getattr(args, 'skip_all_users', False)
        self.verbose = getattr(args, 'verbose', False)
        # control verbose per-user diagnostics (off by default)
        self.show_user_diagnostics = getattr(args, 'show_user_diagnostics', False)
        # control progress bar display (on by default)
        self.show_progress = not getattr(args, 'no_progress', False)
        self.dual_artifacts_path = getattr(args, 'dual_memory_artifacts', None)
        self.dual_threshold = getattr(args, 'dual_memory_threshold', None)
        # inference-time calibration and fallback controls
        self.base_temperature = float(getattr(args, 'base_temperature', 1.0))
        self.side_temperature = float(getattr(args, 'side_temperature', 1.2))
        self.use_entropy_fallback = bool(getattr(args, 'use_entropy_fallback', False))
        self.conf_fallback_threshold = float(getattr(args, 'conf_fallback_threshold', 0.85))
        self.min_unique_ratio = float(getattr(args, 'min_unique_ratio', 0.3))

        # runtime placeholders
        self.model_wrapper = None
        self.dual_runtime = None
        self.evaluator = None
        self.results = {}

        # target metrics (used by evaluator) - include recall to ensure Recall@K is computed
        self.target_metrics = []
        for k in self.k_values:
            self.target_metrics.extend([f'hit@{k}', f'ndcg@{k}', f'recall@{k}'])


        

    def initialize_model(self, model_path=None):
        """Centralized model loading shim.

        Previously evaluate_datasets implemented a large initialize_model which handled
        many checkpoint formats and dual-memory artifacts. That logic now lives in
        src.model_wrapper.P5ModelWrapper._load_model. Here we call the wrapper and
        attach the resulting model/tokenizer to the evaluator.
        """
        if model_path is None:
            model_path = self.model_path

        # If caller requested to load a dual-memory artifact directly, defer to the
        # specialized initializer which constructs a DualMemoryRuntime.
        if self.dual_artifacts_path:
            self.initialize_dual_runtime(self.dual_artifacts_path, self.dual_threshold)
            return

        logger.info("é€šè¿‡ P5ModelWrapper åŠ è½½æ¨¡å‹ (centralized)")
        device = os.environ.get('P5_DEVICE') or ('cuda' if torch.cuda.is_available() else 'cpu')

        # If user passed a directory, try to locate a checkpoint inside or load as HF model dir
        if model_path and os.path.isdir(model_path):
            # common checkpoint candidates
            candidates = [
                os.path.join(model_path, os.path.basename(model_path) + '.pt'),
                #os.path.join(model_path, 'pytorch_model.bin'),
                #os.path.join(model_path, 'model.safetensors'),
            ]
            found = None
            for c in candidates:
                if os.path.exists(c):
                    found = c
                    break
            if found:
                logger.info("æ£€æµ‹åˆ°æ¨¡å‹ç›®å½•ï¼Œä½¿ç”¨å†…éƒ¨ checkpoint: %s", found)
                # prefer to load tokenizer from the directory where checkpoint was found
                checkpoint_path = found
                t5_local_dir = model_path
                # set model_path to the checkpoint for downstream loader
                model_path = checkpoint_path
            else:
                # attempt to load as HuggingFace model directory (contains config/tokenizer files)
                try:
                    logger.info("å°è¯•å°†ç›®å½•ä½œä¸º HuggingFace æ¨¡å‹åŠ è½½: %s", model_path)
                    tokenizer = T5Tokenizer.from_pretrained(model_path)
                    model = T5ForConditionalGeneration.from_pretrained(model_path)
                    # normalize device to torch.device and move model
                    torch_device = torch.device(device)
                    # Ensure model embeddings cover tokenizer vocab (avoid piece id out of range)
                    try:
                        tok_size = len(tokenizer)
                        emb_size = model.get_input_embeddings().weight.size(0)
                        if tok_size > emb_size:
                            logger.info("æ£€æµ‹åˆ° tokenizer vocab (%d) > model embeddings (%d), é‡æ–°è°ƒæ•´æ¨¡å‹ embedding å¤§å°...", tok_size, emb_size)
                            model.resize_token_embeddings(tok_size)
                    except Exception as e:
                        logger.warning("åœ¨åŒæ­¥ tokenizer ä¸æ¨¡å‹ vocab å¤§å°æ—¶å‘ç”Ÿå¼‚å¸¸: %s", e)

                    model = model.to(torch_device)
                    wrapper = SimpleNamespace(model=model, tokenizer=tokenizer, model_path=model_path)
                    wrapper.device = torch_device
                    self.model_wrapper = wrapper
                    self.model = model
                    self.tokenizer = tokenizer
                    self.wise_editor = getattr(self.model, 'wise_editor_ref', None)
                    self.evaluator = P5RecommendationEvaluator()
                    return wrapper
                except Exception as e:
                    logger.warning("æ— æ³•å°†ç›®å½•ä½œä¸º HuggingFace æ¨¡å‹ç›´æ¥åŠ è½½: %s", e)

        # fallback: pass path (file) to P5ModelWrapper which expects a checkpoint file
        # If we detected a local model dir with tokenizer earlier, pass t5_local_dir
        try:
            wrapper = P5ModelWrapper(model_path=model_path, device=device, t5_local_dir=locals().get('t5_local_dir', None))
        except TypeError:
            # older wrapper signature fallback
            wrapper = P5ModelWrapper(model_path=model_path, device=device)
        self.model_wrapper = wrapper
        self.model = wrapper.model
        self.tokenizer = wrapper.tokenizer
        self.wise_editor = getattr(self.model, 'wise_editor_ref', None)
        # ensure evaluator exists
        self.evaluator = P5RecommendationEvaluator()
        return wrapper

    def _canonical_metric_name(self, metric: str) -> str:
        """Normalize metric names so aliases like 'hit_rate@10' map to 'hit@10'."""
        if not metric:
            return metric
        m = metric.lower()
        # normalize hit_rate@K -> hit@K
        if m.startswith('hit_rate@'):
            k = m.split('@', 1)[1]
            return f'hit@{k}'
        # already canonical if hit@, ndcg@, recall@
        if m.startswith('hit@') or m.startswith('ndcg@') or m.startswith('recall@'):
            return m
        return metric

    def load_and_split_data(self) -> Dict[str, Any]:
        """Load the ML1M interaction file and return a standardized data_info dict

        The returned dict contains keys expected by the evaluation pipeline:
         - train_df, test_df: pandas DataFrame with mapped_user_id and mapped_item_id
         - mappings: {'user_to_mapped': {...}, 'mapped_to_user': {...}}
         - all_users, retain_users, forget_users: lists of mapped user ids
         - eval_retain_users, eval_forget_users: lists (may be empty)
        """
        import pandas as pd

        data_dir = os.path.join(PROJECT_ROOT, 'data', 'ML1M')
        inter_file = os.path.join(data_dir, 'ml-1m.inter')
        user_index_file = os.path.join(data_dir, 'user_indexing.txt')

        if not os.path.exists(inter_file):
            raise FileNotFoundError(f"äº¤äº’æ–‡ä»¶ä¸å­˜åœ¨: {inter_file}")
        if not os.path.exists(user_index_file):
            raise FileNotFoundError(f"ç”¨æˆ·ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {user_index_file}")

        # load mappings
        user_map = {line.strip().split()[0]: line.strip().split()[1] for line in open(user_index_file, 'r', encoding='utf-8')}
        mapped_to_user = {v: k for k, v in user_map.items()}

        # load interactions
        # ml-1m.inter may not have a header; ensure consistent column names
        df = pd.read_csv(inter_file, sep='\t', header=None, names=['user_id', 'item_id', 'rating', 'timestamp'], dtype={'user_id': str, 'item_id': str, 'rating': str, 'timestamp': str})
        # normalize whitespace
        df['user_id'] = df['user_id'].astype(str).str.strip()
        df['item_id'] = df['item_id'].astype(str).str.strip()
        # map users
        df['mapped_user_id'] = df['user_id'].map(user_map)
        df.dropna(subset=['mapped_user_id'], inplace=True)
        # ensure string typed ids
        df['mapped_user_id'] = df['mapped_user_id'].astype(str)
        df['mapped_item_id'] = df['item_id'].astype(str)
        # ensure rating and timestamp are numeric for comparisons and sorting
        df['rating'] = pd.to_numeric(df['rating'], errors='coerce').fillna(0).astype(int)
        df['timestamp'] = pd.to_numeric(df['timestamp'], errors='coerce').fillna(0).astype(int)

        # split train/test 80/20 by time
        df = df.sort_values('timestamp')
        split_point = int(len(df) * 0.8)
        train_df = df.iloc[:split_point].copy()
        test_df = df.iloc[split_point:].copy()

        all_train_users = sorted(train_df['mapped_user_id'].unique().astype(str).tolist())

        # try to load precomputed forget/retain user lists if available
        forget_users = []
        retain_users = []
        forget_path = os.path.join(PROJECT_ROOT, 'results', 'forget_samples_subset.json')
        retain_path = os.path.join(PROJECT_ROOT, 'results', 'retain_samples_subset.json')
        if os.path.exists(forget_path):
            with open(forget_path, 'r', encoding='utf-8') as f:
                forget_samples = json.load(f)
            # map original ids to mapped ids using user_map
            forget_users = []
            for s in forget_samples:
                orig = str(s.get('user_id'))
                mapped = user_map.get(orig)
                if mapped:
                    forget_users.append(mapped)
        if os.path.exists(retain_path):
            with open(retain_path, 'r', encoding='utf-8') as f:
                retain_samples = json.load(f)
            retain_users = []
            for s in retain_samples:
                orig = str(s.get('user_id'))
                mapped = user_map.get(orig)
                if mapped:
                    retain_users.append(mapped)

        # fallback default sets
        if not retain_users:
            retain_users = [u for u in all_train_users if u not in set(forget_users)]

        # prepare evaluation subsets (empty by default)
        eval_retain_users = list(retain_users)[:self.eval_sample_size]
        eval_forget_users = list(forget_users)[:self.eval_sample_size]

        mappings = {'user_to_mapped': user_map, 'mapped_to_user': mapped_to_user}

        data_info = {
            'train_df': train_df,
            'test_df': test_df,
            'mappings': mappings,
            'all_users': all_train_users,
            'retain_users': retain_users,
            'forget_users': forget_users,
            'eval_retain_users': eval_retain_users,
            'eval_forget_users': eval_forget_users,
        }
        logger.info(f"æ•°æ®åŠ è½½å®Œæˆ: train={len(train_df)} rows, test={len(test_df)} rows, users={len(all_train_users)}")
        return data_info
        

    def initialize_dual_runtime(self, artifact_path: str, threshold_override: Optional[float] = None) -> None:
        """åŠ è½½DualMemoryæ¨ç†è¿è¡Œæ—¶ï¼Œç”¨äºä¸»è®°å¿†+ä¾§è®°å¿†+è·¯ç”±çš„å®Œæ•´æ¨¡å‹ã€‚"""
        if not artifact_path:
            raise ValueError("Dual-memory artifact path is required")
        logger.info("ğŸ”§ åˆå§‹åŒ–DualMemoryè¿è¡Œæ—¶: %s", artifact_path)
        if not os.path.exists(artifact_path):
            raise FileNotFoundError(f"Dual-memory artifacts æ–‡ä»¶ä¸å­˜åœ¨: {artifact_path}")

        device = os.environ.get('P5_DEVICE') or ('cuda' if torch.cuda.is_available() else 'cpu')
        runtime = load_dual_memory_runtime(artifact_path, device=device, threshold=threshold_override)
        self.dual_runtime = runtime
        device_str = str(runtime.device)
        self.model_wrapper = SimpleNamespace(
            tokenizer=runtime.tokenizer,
            model=runtime.model,
            device=device_str,
        )
        self.evaluator = P5RecommendationEvaluator()

        self.dual_artifact_metadata = {}
        try:
            payload = torch.load(artifact_path, map_location='cpu')
            router_metrics = payload.get('router_metrics') or {}
            router_threshold = float(payload.get('router_threshold', runtime.threshold))
            epsilon_val = payload.get('router_feature_stats', {}).get('epsilon', runtime.epsilon)
            precision_val = router_metrics.get('precision')
            recall_val = router_metrics.get('recall')
            precision_str = f"{precision_val:.3f}" if isinstance(precision_val, (int, float)) else str(precision_val)
            recall_str = f"{recall_val:.3f}" if isinstance(recall_val, (int, float)) else str(recall_val)
            epsilon_float = float(epsilon_val) if epsilon_val is not None else float('nan')
            self.dual_artifact_metadata = {
                'router_metrics': router_metrics,
                'router_threshold': router_threshold,
                'epsilon': epsilon_val,
            }
            logger.info(
                "âœ… Dual-memoryè·¯ç”±æŒ‡æ ‡: threshold=%.4f, epsilon=%.4f, precision=%s, recall=%s",
                router_threshold,
                epsilon_float,
                precision_str,
                recall_str,
            )
        except Exception as exc:
            logger.warning("âš ï¸ æ— æ³•è§£æDual-memoryè·¯ç”±ç»Ÿè®¡ä¿¡æ¯: %s", exc)

        logger.info("ğŸš€ Dual-memory runtime å‡†å¤‡å®Œæˆ (device=%s, threshold=%.4f)", device_str, float(runtime.threshold))
            
    def evaluate_model_performance(self, data_info: Dict, user_set_info: Dict, set_name: str = "all", save_recs: bool = False) -> Dict[str, float]:
            """
            [æœ€ç»ˆä¿®å¤ç‰ˆ]
            è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚
            1. "ä¿ç•™æ•ˆç”¨" ä½¿ç”¨æµ‹è¯•é›†è¿›è¡Œè¯„ä¼° (é¢„æµ‹æœªæ¥)ã€‚
            2. "é—å¿˜æ•ˆèƒ½" ç›´æ¥ä½¿ç”¨å·²çŸ¥çš„é—å¿˜åˆ—è¡¨è¿›è¡Œè¯„ä¼° (æ£€éªŒè¿‡å»)ã€‚
            """
            if isinstance(user_set_info, dict):
                user_set = {str(uid) for uid in user_set_info.get('users', [])}
                # unlearning_requests is now a dict: {user_id: [item1, item2]}
                unlearning_requests = user_set_info.get('unlearning_requests', {})
            else:
                # Fallback for old format
                user_set = {str(uid) for uid in user_set_info}
                unlearning_requests = {}

            logger.info(f"ğŸ“Š [æœ€ç»ˆä¿®å¤ç‰ˆè·¯ç”±] è¯„ä¼°æ¨¡å‹åœ¨ {set_name} é›†ä¸Šçš„æ€§èƒ½ (ç”¨æˆ·æ•°: {len(user_set)})...")
            
            train_df = data_info['train_df']
            test_df = data_info['test_df']

            # 1. [é€»è¾‘ä¸å˜] "ä¿ç•™æ•ˆç”¨"çš„æ­£ç¡®ç­”æ¡ˆæ¥è‡ªæµ‹è¯•é›† (è¡¡é‡å¯¹æœªæ¥çš„é¢„æµ‹èƒ½åŠ›)
            gt_retained = defaultdict(set)
            for user_id in user_set:
                user_test_data = test_df[test_df['mapped_user_id'] == user_id]
                if not user_test_data.empty:
                    items = set(map(str, user_test_data[user_test_data['rating'] >= 4]['mapped_item_id'].tolist()))
                    gt_retained[user_id] = items

            # 2. [æ ¸å¿ƒé€»è¾‘ä¿®æ”¹] "é—å¿˜æ•ˆèƒ½"çš„æ­£ç¡®ç­”æ¡ˆç›´æ¥æ¥è‡ª unlearning_requests (è¡¡é‡å¯¹è¿‡å»äº¤äº’çš„é—å¿˜èƒ½åŠ›)
            gt_forgotten = defaultdict(set)
            if unlearning_requests:
                for user_id, items_to_forget in unlearning_requests.items():
                    # Only consider users who are part of the current evaluation set
                    if user_id in user_set:
                        gt_forgotten[user_id] = set(map(str, items_to_forget))

            predictions = []
            ground_truth_retain_list, ground_truth_forget_list = [], []
            processed_users_count = 0
            routed_to_side_count = 0
            
            dual_runtime = getattr(self, 'dual_runtime', None)
            
            user_list_to_eval = sorted(list(user_set))

            # choose an iterator: tqdm if available and requested, else plain list
            if self.show_progress and tqdm is not None:
                iterator = tqdm(user_list_to_eval, desc=f"Evaluating {set_name}", unit="user")
            else:
                iterator = user_list_to_eval

            for mapped_user_id in iterator:
                # Wrap the entire per-user processing in a try/except so a single failing user
                # doesn't abort the whole evaluation run. Any exceptions will be logged with
                # prompt and traceback for debugging.
                try:
                    user_history = train_df[train_df['mapped_user_id'] == mapped_user_id]['mapped_item_id'].astype(str).tolist()
                    if len(user_history) < 2:
                        # not enough history to evaluate
                        continue

                    # Use a consistent history prompt for both original and unlearned models
                    # The prompt should represent the state *before* the items were forgotten
                    items_to_forget_for_user = gt_forgotten.get(mapped_user_id, set())
                    full_history_for_prompt = user_history + list(items_to_forget_for_user)
                    history_str = " ".join(f"item_{item}" for item in full_history_for_prompt[-20:]) if full_history_for_prompt else "<empty>"
                    prompt = f"User {mapped_user_id} recent history: {history_str}. Recommend next item."

                    is_forget_user = mapped_user_id in unlearning_requests
                    use_side_memory = is_forget_user and (dual_runtime is not None)

                    if use_side_memory:
                        dual_runtime.adapter.set_mode("side")
                        routed_to_side_count += 1
                    elif dual_runtime:
                        dual_runtime.adapter.set_mode("main")

                    # tokenize and move tensors to model device
                    inputs = self.model_wrapper.tokenizer(prompt, return_tensors="pt")
                    torch_device = getattr(self.model_wrapper, 'device', ('cuda' if torch.cuda.is_available() else 'cpu'))
                    inputs = {k: v.to(torch_device) for k, v in inputs.items()}

                    num_return_sequences = getattr(self.args, 'num_return_sequences', 10)
                    num_beams = getattr(self.args, 'num_beams', 10)
                    max_gen_len = getattr(self.args, 'max_gen_len', 150)

                    # Ensure we request at least max_k sequences so de-duplication can still
                    # produce up to K unique recommendations without relying on fallback.
                    max_k = max(self.k_values) if hasattr(self, 'k_values') else 20
                    if num_return_sequences < max_k:
                        if self.verbose:
                            logger.debug("è°ƒæ•´ generate.num_return_sequences: %d -> %d (ä»¥æ»¡è¶³ max_k=%d)", num_return_sequences, max_k, max_k)
                        num_return_sequences = max_k

                    # transformers requires num_return_sequences <= num_beams when using beam search.
                    # If user set num_beams smaller, bump it up to avoid ValueError.
                    if num_beams < num_return_sequences:
                        if self.verbose:
                            logger.debug("è°ƒæ•´ generate.num_beams: %d -> %d (ç¡®ä¿ num_beams >= num_return_sequences)", num_beams, num_return_sequences)
                        num_beams = num_return_sequences

                    # helper: temperature-scaled generation with optional score outputs
                    try:
                        from transformers.generation.logits_process import LogitsProcessor, LogitsProcessorList
                    except Exception:
                        LogitsProcessor = object  # type: ignore
                        class LogitsProcessorList(list):  # type: ignore
                            pass

                    class TemperatureLogitsProcessor(LogitsProcessor):  # type: ignore
                        def __init__(self, temperature: float) -> None:
                            self.t = max(1e-6, float(temperature))
                        def __call__(self, input_ids, scores):
                            return scores / self.t

                    class TokenPenaltyProcessor(LogitsProcessor):  # type: ignore
                        def __init__(self, banned_token_ids: set, penalty: float = 20.0) -> None:
                            self.banned = set(int(x) for x in banned_token_ids)
                            self.penalty = float(max(0.0, penalty))
                        def __call__(self, input_ids, scores):
                            if not self.banned or self.penalty <= 0:
                                return scores
                            try:
                                vocab = scores.size(-1)
                                import torch as _torch
                                idx = _torch.tensor(list(self.banned), device=scores.device, dtype=_torch.long)
                                valid = (idx >= 0) & (idx < vocab)
                                idx = idx[valid]
                                if idx.numel() > 0:
                                    scores.index_fill_(dim=-1, index=idx, value=(scores.min().item() - self.penalty))
                            except Exception:
                                pass
                            return scores

                    def _build_processors(temp: float, penalty_tokens: Optional[set] = None, penalty_val: float = 0.0):
                        procs = LogitsProcessorList()
                        if temp and abs(float(temp) - 1.0) > 1e-6:
                            procs.append(TemperatureLogitsProcessor(float(temp)))
                        if penalty_tokens and penalty_val > 0.0:
                            procs.append(TokenPenaltyProcessor(set(penalty_tokens), float(penalty_val)))
                        return procs

                    def _encode_bad_words(_tokenizer, forbidden_items):
                        seqs = []
                        first_tokens = set()
                        for it in forbidden_items:
                            text = f"item_{it}"
                            try:
                                ids = _tokenizer.encode(text, add_special_tokens=False)
                            except Exception:
                                ids = []
                            if ids:
                                seqs.append(ids)
                                first_tokens.add(ids[0])
                        return seqs, first_tokens

                    def _generate_with_temp(_model, _tokenizer, _inputs: dict, temp: float,
                                             bad_words_ids: Optional[list] = None,
                                             penalty_tokens: Optional[set] = None,
                                             penalty_val: float = 0.0):
                        processors = _build_processors(temp, penalty_tokens=penalty_tokens, penalty_val=penalty_val)
                        with torch.no_grad():
                            out = _model.generate(
                                **_inputs,
                                max_length=max_gen_len,
                                num_return_sequences=num_return_sequences,
                                num_beams=num_beams,
                                do_sample=False,
                                early_stopping=True,
                                logits_processor=processors,
                                bad_words_ids=bad_words_ids,
                                return_dict_in_generate=True,
                                output_scores=True,
                            )
                        decoded_local = self.model_wrapper.tokenizer.batch_decode(out.sequences, skip_special_tokens=True)
                        # compute normalized entropy from first step
                        norm_ent = None
                        try:
                            if out.scores and len(out.scores) > 0:
                                first_scores = out.scores[0]
                                p = torch.softmax(first_scores, dim=-1)
                                H = -(p * (p.clamp_min(1e-12).log())).sum(dim=-1)
                                V = p.size(-1)
                                norm_ent = float((H / math.log(V)).mean().item())
                        except Exception:
                            norm_ent = None
                        return decoded_local, norm_ent

                    # choose temperature by route
                    temp_to_use = self.side_temperature if use_side_memory else self.base_temperature
                    # prepare inputs once
                    gen_inputs = {k: v.to(torch_device) for k, v in inputs.items()}
                    # certified-forgetful decoding: build bad_words_ids & penalties only for forget users on side
                    bad_words_ids = None
                    penalty_tokens = None
                    penalty_val = 0.0
                    if use_side_memory and bool(getattr(self.args, 'certified_forgetful_decoding', False)):
                        if items_to_forget_for_user:
                            bad_words_ids, penalty_tokens = _encode_bad_words(self.model_wrapper.tokenizer, list(items_to_forget_for_user))
                            penalty_val = float(getattr(self.args, 'forbidden_penalty', 20.0))

                    generated_texts, norm_entropy = _generate_with_temp(
                        self.model_wrapper.model,
                        self.model_wrapper.tokenizer,
                        gen_inputs,
                        temp_to_use,
                        bad_words_ids=bad_words_ids,
                        penalty_tokens=penalty_tokens,
                        penalty_val=penalty_val,
                    )

                    recommended_items = []
                    seen_items = set()
                    item_pattern = re.compile(r'item_(\d+)', re.IGNORECASE)
                    for text in generated_texts:
                        items = item_pattern.findall(text)
                        for item in items:
                            if item not in seen_items:
                                recommended_items.append(item)
                                seen_items.add(item)

                    # ========== è¯¦ç»†è¯Šæ–­ï¼ˆå¯å¸®åŠ©å®šä½ tokenization / vocab mismatch / è¶Šç•Œé—®é¢˜ï¼‰ ==========
                    try:
                        # tokenization of the input prompt
                        tokenized_prompt = self.model_wrapper.tokenizer.tokenize(prompt)
                        tokenized_ids = self.model_wrapper.tokenizer(prompt, return_tensors='pt')['input_ids'][0].tolist()

                        # model embedding table size (rows)
                        emb_rows = None
                        try:
                            emb = self.model_wrapper.model.get_input_embeddings()
                            emb_rows = emb.weight.size(0)
                        except Exception:
                            emb_rows = None

                        # count how many generated sequences contained at least one item_{id}
                        total_generated = len(generated_texts)
                        generated_with_match = sum(1 for t in generated_texts if item_pattern.search(t))

                        diag = {
                            'user_id': mapped_user_id,
                            'prompt_tokens': tokenized_prompt,
                            'prompt_token_ids': tokenized_ids,
                            'tokenizer_vocab_size': len(self.model_wrapper.tokenizer) if hasattr(self.model_wrapper, 'tokenizer') else None,
                            'model_embedding_rows': emb_rows,
                            'generated_sequences': total_generated,
                            'generated_with_item_match': generated_with_match,
                            'unique_recommended_items': len(recommended_items),
                            'sample_recommended_items': recommended_items[:10]
                        }

                        # detect any out-of-range ids in prompt tokens
                        out_of_range_ids = []
                        if emb_rows is not None:
                            for tid in tokenized_ids:
                                if isinstance(tid, int) and tid >= emb_rows:
                                    out_of_range_ids.append(int(tid))

                        # also check generated token ids (best-effort): decode back to ids
                        try:
                            # we can attempt to decode generated_texts into ids via tokenizer
                            gen_ids = [self.model_wrapper.tokenizer(text, return_tensors='pt')['input_ids'][0].tolist() for text in generated_texts]
                            gen_out_of_range = []
                            if emb_rows is not None:
                                for gid_list in gen_ids:
                                    for gid in gid_list:
                                        if gid >= emb_rows:
                                            gen_out_of_range.append(int(gid))
                            if gen_out_of_range:
                                out_of_range_ids.extend(gen_out_of_range)
                        except Exception:
                            # best-effort only; ignore if tokenizer can't re-tokenize snippets
                            pass

                        if out_of_range_ids:
                            diag['out_of_range_ids'] = sorted(set(out_of_range_ids))

                        # attach diag to results for later inspection and log at debug level
                        self.results.setdefault('per_user_diagnostics', []).append(diag)
                        if self.show_user_diagnostics:
                            logger.debug("[EVAL DIAG FULL] user=%s diag=%s", mapped_user_id, {k: diag.get(k) for k in ('tokenizer_vocab_size','model_embedding_rows','generated_sequences','generated_with_item_match','unique_recommended_items','out_of_range_ids')})
                    except Exception as _diag_exc:
                        # Do not fail evaluation because diagnostics failed
                        if self.show_user_diagnostics:
                            logger.debug("[EVAL DIAG ERROR] æ— æ³•ç”Ÿæˆè¯Šæ–­ä¿¡æ¯ user=%s error=%s", mapped_user_id, _diag_exc)

                    # ========== è¯Šæ–­è¾“å‡ºï¼šè®°å½•æ¯ä¸ªç”¨æˆ·çš„æ¨èæ•°é‡ä¸æ ·ä¾‹ ==========
                    max_k = max(self.k_values) if hasattr(self, 'k_values') else 20
                    if len(recommended_items) < max_k:
                        # minimal debug output: only log counts when user diagnostics enabled
                        if self.show_user_diagnostics:
                            logger.debug("[EVAL DIAG] user=%s num_unique_recs=%d (less than max_k=%d). Generated_texts_count=%d.", mapped_user_id, len(recommended_items), max_k, len(generated_texts))

                    # entropy/uniqueness-based fallback: for forget users routed to side
                    trigger_fallback = False
                    if use_side_memory and self.use_entropy_fallback:
                        if norm_entropy is not None and self.conf_fallback_threshold is not None:
                            if float(norm_entropy) > float(self.conf_fallback_threshold):
                                trigger_fallback = True
                        unique_ratio = (len(recommended_items) / float(max_k)) if max_k > 0 else 0.0
                        if unique_ratio < float(self.min_unique_ratio):
                            trigger_fallback = True

                    if trigger_fallback and dual_runtime:
                        # regenerate with main (base) at base temperature
                        dual_runtime.adapter.set_mode("main")
                        gen_inputs_fb = {k: v.to(torch_device) for k, v in inputs.items()}
                        generated_texts, _ = _generate_with_temp(self.model_wrapper.model, self.model_wrapper.tokenizer, gen_inputs_fb, self.base_temperature)
                        # rebuild items
                        recommended_items = []
                        seen_items = set()
                        for text in generated_texts:
                            items = item_pattern.findall(text)
                            for item in items:
                                if item not in seen_items:
                                    recommended_items.append(item)
                                    seen_items.add(item)
                                if len(recommended_items) >= max_k:
                                    break
                            if len(recommended_items) >= max_k:
                                break

                    # ========== å›é€€ç­–ç•¥ï¼šè‹¥å»é‡åå€™é€‰è¿‡å°‘ï¼Œåˆ™ç”¨è®­ç»ƒé›†æµè¡Œç‰©å“è¡¥é½ï¼Œé¿å… top-K å…¨ä¸º 0 çš„è¯¯å¯¼æ€§ç»“æœ ==========
                    # Fallback strategy: optionally pad with popular items from training set
                    used_fallback = False
                    if len(recommended_items) < max_k and not getattr(self.args, 'disable_fallback', False):
                        try:
                            popular = list(train_df['mapped_item_id'].value_counts().index.astype(str))
                        except Exception:
                            popular = []
                        for p in popular:
                            if p not in seen_items:
                                recommended_items.append(p)
                                seen_items.add(p)
                                used_fallback = True
                            if len(recommended_items) >= max_k:
                                break
                    if used_fallback:
                        # track fallback usage in results dict
                        self.results.setdefault('fallback_count', 0)
                        self.results['fallback_count'] += 1

                    if self.show_user_diagnostics:
                        logger.debug("[EVAL DIAG] user=%s final_rec_count=%d sample_recs=%s", mapped_user_id, len(recommended_items), recommended_items[:10])

                    # Certified check & certificate emission
                    if use_side_memory and bool(getattr(self.args, 'emit_certificates', False)):
                        try:
                            forb_set = set(map(str, items_to_forget_for_user)) if items_to_forget_for_user else set()
                            inter = [x for x in recommended_items if x in forb_set]
                            import time as _time, os as _os, json as _json
                            _os.makedirs('results/certificates', exist_ok=True)
                            cert = {
                                'time': int(_time.time()),
                                'user_id': mapped_user_id,
                                'route': 'side' if use_side_memory else 'main',
                                'forbidden_items': list(forb_set),
                                'topk_items': recommended_items[:max_k],
                                'violation_count': len(inter),
                                'violations': inter,
                            }
                            with open(_os.path.join('results/certificates', f'cert_{mapped_user_id}_{int(_time.time())}.json'), 'w', encoding='utf-8') as f:
                                _json.dump(cert, f, ensure_ascii=False, indent=2)
                        except Exception:
                            pass

                    predictions.append({'user_id': mapped_user_id, 'recommended_items': recommended_items})

                    # For each user, populate the ground truth lists for the two separate evaluations
                    for item_id in gt_retained.get(mapped_user_id, set()):
                        ground_truth_retain_list.append({'user_id': mapped_user_id, 'item_id': item_id})
                    for item_id in gt_forgotten.get(mapped_user_id, set()):
                        ground_truth_forget_list.append({'user_id': mapped_user_id, 'item_id': item_id})

                    processed_users_count += 1
                except Exception as e:
                    # Log detailed debug info for the failing user/prompt
                    import traceback
                    tb = traceback.format_exc()
                    logger.error("âŒ ç”¨æˆ·è¯„ä¼°æ—¶å‘ç”Ÿå¼‚å¸¸ user=%s prompt=%s error=%s", mapped_user_id, locals().get('prompt', '<no-prompt>'), e)
                    logger.debug("Traceback:\n%s", tb)
                    # add minimal placeholder to keep evaluations moving
                    predictions.append({'user_id': mapped_user_id, 'recommended_items': []})
                    # continue with next user
                    continue

            if dual_runtime:
                dual_runtime.adapter.set_mode("main")

            if processed_users_count == 0:
                logger.warning(f"{set_name}é›†æ²¡æœ‰å¯è¯„ä¼°çš„ç”¨æˆ·ã€‚")
                return {}
            
            if dual_runtime:
                logger.info("=" * 60)
                logger.info(f"ç¡®å®šæ€§è·¯ç”±å†³ç­–åˆ†æ ({set_name}):")
                logger.info(f"  æ€»è®¡ {routed_to_side_count} / {processed_users_count} ({routed_to_side_count/processed_users_count:.2%}) ä¸ªç”¨æˆ·è¢«è·¯ç”±åˆ°ä¾§è®°å¿†ã€‚")
                logger.info("=" * 60)

            final_metrics = {}
            if ground_truth_retain_list:
                retain_metrics = self.evaluator.evaluate_recommendations(predictions, ground_truth_retain_list, self.target_metrics)
                # compute precision@k and user-averaged recall@k
                precision_metrics = {}
                user_avg_recall_metrics = {}
                for k in self.k_values:
                    # precision@k = total_hits_on_topk / (k * num_users_with_preds)
                    # We'll compute precision as hits / (k * N_users) where N_users is users with gt
                    prec = self._precision_at_k(predictions, ground_truth_retain_list, k)
                    user_avg_rec = self._user_averaged_recall(predictions, ground_truth_retain_list, k)
                    precision_metrics[f'precision@{k}'] = prec
                    user_avg_recall_metrics[f'user_avg_recall@{k}'] = user_avg_rec

                logger.info(f"ğŸ“ˆ {set_name} - ä¿ç•™æ•ˆç”¨ (Retain Utility):")
                for metric, value in {**retain_metrics, **precision_metrics, **user_avg_recall_metrics}.items():
                    logger.info(f"  {metric}: {value:.4f}")
                    final_metrics[f"{metric}_retain"] = value
            else:
                logger.info(f"â„¹ï¸ {set_name} - æ— éœ€è¯„ä¼°çš„ä¿ç•™é¡¹ã€‚")
            
            if ground_truth_forget_list:
                forget_target_metrics = []
                for k in self.k_values:
                    forget_target_metrics.extend([f'hit@{k}', f'ndcg@{k}', f'recall@{k}'])
                
                forget_metrics = self.evaluator.evaluate_recommendations(predictions, ground_truth_forget_list, forget_target_metrics)
                # also compute precision & user-avg recall for forget set
                precision_metrics_f = {}
                user_avg_recall_metrics_f = {}
                for k in self.k_values:
                    precision_metrics_f[f'precision@{k}'] = self._precision_at_k(predictions, ground_truth_forget_list, k)
                    user_avg_recall_metrics_f[f'user_avg_recall@{k}'] = self._user_averaged_recall(predictions, ground_truth_forget_list, k)

                logger.info(f"ğŸ“‰ {set_name} - é—å¿˜æ•ˆèƒ½ (Forget Efficacy):")
                for metric, value in {**forget_metrics, **precision_metrics_f, **user_avg_recall_metrics_f}.items(): 
                    logger.info(f"  {metric}_forgotten: {value:.4f} (æ­¤å€¼è¶Šä½ï¼Œé—å¿˜æ•ˆæœè¶Šå¥½)")
                    final_metrics[f"{metric}_forgotten"] = value
            else:
                logger.info(f"â„¹ï¸ {set_name} - æ— éœ€è¯„ä¼°çš„é—å¿˜é¡¹ã€‚")
            
            return final_metrics

    def _precision_at_k(self, predictions: list, ground_truth: list, k: int) -> float:
        """Compute global precision@k = total_hits_on_topk / (k * num_users_with_gt)"""
        # build maps
        user_preds = self.evaluator._build_user_predictions(predictions)
        user_gt = self.evaluator._build_user_ground_truth(ground_truth)
        total_hits = 0
        total_possible = 0
        for uid, gt_items in user_gt.items():
            if not gt_items:
                continue
            preds = user_preds.get(uid, [])[:k]
            hits = sum(1 for it in preds if it in set(gt_items))
            total_hits += hits
            total_possible += k
        if total_possible == 0:
            return 0.0
        return total_hits / total_possible

    def _user_averaged_recall(self, predictions: list, ground_truth: list, k: int) -> float:
        """Compute user-averaged recall: mean_u |Pred_u@k âˆ© GT_u| / |GT_u|"""
        user_preds = self.evaluator._build_user_predictions(predictions)
        user_gt = self.evaluator._build_user_ground_truth(ground_truth)
        recalls = []
        for uid, gt_items in user_gt.items():
            gt_set = set(gt_items)
            if not gt_set:
                continue
            preds = user_preds.get(uid, [])[:k]
            hits = sum(1 for it in preds if it in gt_set)
            recalls.append(hits / len(gt_set))
        if not recalls:
            return 0.0
        return float(sum(recalls) / len(recalls))

    def run_evaluation(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®é›†è¯„ä¼°"""
        if self.compare_mode:
            return self.run_comparison_evaluation()
        else:
            return self.run_single_evaluation()
    
    def run_single_evaluation(self):
        """è¿è¡Œå•ä¸ªæ¨¡å‹è¯„ä¼°"""
        logger.info(f"ğŸš€ å¼€å§‹P5æ¨èæ¨¡å‹è¯„ä¼°: {self.model_path}")
        
        try:
            # 1. åˆå§‹åŒ–æ¨¡å‹
            self.initialize_model()
            
            # 2. åŠ è½½å’Œåˆ’åˆ†æ•°æ®
            data_info = self.load_and_split_data()
            
            # 3. è¯„ä¼°æ¨¡å‹æ€§èƒ½
            logger.info("\n" + "=" * 80)
            logger.info("ğŸ“Š æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¯„ä¼°")
            logger.info("=" * 80)
            
            # è¯„ä¼°å…¨ä½“ç”¨æˆ·æ€§èƒ½ï¼ˆå¯è·³è¿‡ä»¥åŠ é€Ÿï¼‰
            if not getattr(self.args, 'skip_all_users', False):
                logger.info("\nğŸ” è¯„ä¼°å…¨ä½“ç”¨æˆ·æ€§èƒ½:")
                self.results['all_users'] = self.evaluate_model_performance(
                    data_info, data_info['all_users'], "å…¨ä½“ç”¨æˆ·", save_recs=self.save_predictions
                )
            else:
                logger.info("â­ï¸ å·²è·³è¿‡å…¨ä½“ç”¨æˆ·è¯„ä¼° (--skip_all_users)")
            
            # è¯„ä¼°ä¿ç•™é›†æ€§èƒ½
            logger.info("\nğŸ” è¯„ä¼°ä¿ç•™é›†æ€§èƒ½:")
            self.results['retain_set'] = self.evaluate_model_performance(
                data_info, data_info['eval_retain_users'], "ä¿ç•™é›†", save_recs=self.save_predictions
            )
            
            # è¯„ä¼°é—å¿˜é›†æ€§èƒ½
            logger.info("\nğŸ” è¯„ä¼°é—å¿˜é›†æ€§èƒ½:")
            self.results['forget_set'] = self.evaluate_model_performance(
                data_info, data_info['eval_forget_users'], "é—å¿˜é›†", save_recs=self.save_predictions
            )
            
            # 4. æ€§èƒ½åˆ†æ
            self.analyze_performance()
            
            # 5. ä¿å­˜ç»“æœ
            self.save_results(data_info)
            
            logger.info("\nğŸ‰ æ¨¡å‹è¯„ä¼°å®Œæˆ!")
            return True
            
        except Exception as e:
            logger.error(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
            return False

    def run_comparison_evaluation(self):
            """
            [æœ€ç»ˆä¿®å¤ç‰ˆ] è¿è¡Œå¯¹æ¯”è¯„ä¼°ã€‚
            ç›´æ¥ä» forget/retain æ–‡ä»¶ä¸­åŠ è½½ç”¨æˆ·ï¼Œä»¥åŒ¹é…æ–°çš„ã€æ›´çœŸå®çš„é—å¿˜ä»»åŠ¡ã€‚
            """
            logger.info("ğŸš€ å¼€å§‹P5æ¨èæ¨¡å‹å¯¹æ¯”è¯„ä¼°")
            logger.info(f"ğŸ“‹ åŸå§‹æ¨¡å‹: {self.original_model_path}")
            logger.info(f"ğŸ“‹ é—å¿˜åæ¨¡å‹ (å·¥ä»¶è·¯å¾„): {self.unlearned_model_path}")
            
            try:
                # 1. åŠ è½½æ‰€æœ‰å¿…è¦çš„æ•°æ®
                data_info = self.load_and_split_data()
                
                forget_samples_path = os.path.join(PROJECT_ROOT, 'results', 'forget_samples_subset.json')
                if not os.path.exists(forget_samples_path):
                    raise FileNotFoundError(f"é—å¿˜æ ·æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {forget_samples_path}ã€‚")
                
                with open(forget_samples_path, 'r', encoding='utf-8') as f:
                    forget_samples = json.load(f)

                # 2. [æ ¸å¿ƒé€»è¾‘ä¿®å¤] ç›´æ¥ä»å·²ç”Ÿæˆçš„æ–‡ä»¶ä¸­åŠ è½½è¯„ä¼°ç”¨æˆ·ï¼Œä¸å†è¿›è¡Œæ— æ•ˆæœç´¢
                logger.info("ğŸ¯ ç›´æ¥ä» forget_samples.json åŠ è½½è¯„ä¼°ç”¨æˆ·...")
                
                user_map = data_info['mappings']['user_to_mapped']
                
                # a. åŠ è½½æ‰€æœ‰é—å¿˜è¯·æ±‚
                unlearning_requests = defaultdict(list)
                all_forget_user_ids = []
                for sample in forget_samples:
                    user_id = str(sample.get("user_id"))
                    mapped_user_id = user_map.get(user_id)
                    items_to_forget = sample.get("suppression_targets", [])
                    
                    if mapped_user_id and items_to_forget:
                        unlearning_requests[mapped_user_id].extend(map(str, items_to_forget))
                        if mapped_user_id not in all_forget_user_ids:
                            all_forget_user_ids.append(mapped_user_id)

                # b. ä»åŠ è½½çš„ç”¨æˆ·ä¸­æŠ½æ ·
                if len(all_forget_user_ids) < self.eval_sample_size:
                    logger.warning(
                        f"è­¦å‘Šï¼šè¯·æ±‚è¯„ä¼°çš„é—å¿˜ç”¨æˆ·æ•°({self.eval_sample_size}) > æ–‡ä»¶ä¸­å®é™…ç”¨æˆ·æ•°({len(all_forget_user_ids)})ã€‚å°†ä½¿ç”¨å…¨éƒ¨ç”¨æˆ·ã€‚"
                    )
                    eval_forget_users = all_forget_user_ids
                else:
                    eval_forget_users = random.sample(all_forget_user_ids, self.eval_sample_size)

                if not eval_forget_users:
                    raise RuntimeError("è‡´å‘½é”™è¯¯ï¼šä» forget_samples.json ä¸­æœªèƒ½åŠ è½½åˆ°ä»»ä½•æœ‰æ•ˆçš„é—å¿˜ç”¨æˆ·ã€‚")

                # c. ä»ä¿ç•™ç”¨æˆ·åˆ—è¡¨ä¸­æŠ½æ ·ç›¸åŒæ•°é‡çš„ç”¨æˆ·
                eval_retain_users = random.sample(
                    [u for u in data_info['retain_users'] if u not in eval_forget_users],
                    min(len(eval_forget_users), len(data_info['retain_users']))
                )

                logger.info("=" * 60)
                logger.info("ğŸ“Š å…¬å¹³è¯„ä¼°é›†æ„é€ å®Œæˆ:")
                logger.info(f"  é—å¿˜é›†è¯„ä¼°ç”¨æˆ·æ•°: {len(eval_forget_users)}")
                logger.info(f"  ä¿ç•™é›†è¯„ä¼°ç”¨æˆ·æ•°: {len(eval_retain_users)}")
                logger.info("=" * 60)
                
                retain_set_info = {'users': eval_retain_users, 'unlearning_requests': {}}
                forget_set_info = {'users': eval_forget_users, 'unlearning_requests': unlearning_requests}
                    
                # --- 3. åç»­è¯„ä¼°æµç¨‹å®Œå…¨ä¸å˜ ---
                logger.info("\n" + "=" * 80)
                logger.info("ğŸ“Š è¯„ä¼°åŸå§‹æ¨¡å‹æ€§èƒ½")
                self.initialize_model(self.original_model_path)
                original_results = {}
                original_results['retain_set'] = self.evaluate_model_performance(data_info, retain_set_info, "ä¿ç•™é›†(åŸå§‹)")
                original_results['forget_set'] = self.evaluate_model_performance(data_info, forget_set_info, "é—å¿˜é›†(åŸå§‹)")
                    
                logger.info("\n" + "=" * 80)
                logger.info("ğŸ“Š è¯„ä¼°é—å¿˜åæ¨¡å‹æ€§èƒ½")
                self.dual_artifacts_path = self.unlearned_model_path
                self.initialize_dual_runtime(self.dual_artifacts_path, self.dual_threshold)
                unlearned_results = {}
                unlearned_results['retain_set'] = self.evaluate_model_performance(data_info, retain_set_info, "ä¿ç•™é›†(é—å¿˜å)")
                unlearned_results['forget_set'] = self.evaluate_model_performance(data_info, forget_set_info, "é—å¿˜é›†(é—å¿˜å)")
                    
                self.analyze_comparison(original_results, unlearned_results)
                self.save_comparison_results(data_info, original_results, unlearned_results)
                    
                logger.info("\nğŸ‰ å¯¹æ¯”è¯„ä¼°å®Œæˆ!")
                return True
                    
            except Exception as e:
                logger.error(f"âŒ å¯¹æ¯”è¯„ä¼°å¤±è´¥: {e}", exc_info=True)
                return False

    def analyze_comparison(self, original_results: Dict, unlearned_results: Dict):
            """[æœ€ç»ˆç‰ˆ] åˆ†æåŸå§‹æ¨¡å‹ä¸é—å¿˜åæ¨¡å‹çš„æ€§èƒ½å¯¹æ¯”ï¼Œå¹¶æä¾›ç»“æ„åŒ–çš„é—å¿˜æ•ˆæœè§£è¯»ã€‚"""
            logger.info("\n" + "=" * 80)
            logger.info("ğŸ“Š åŸå§‹æ¨¡å‹ vs é—å¿˜åæ¨¡å‹æ€§èƒ½å¯¹æ¯”åˆ†æ")
            logger.info("=" * 80)
            
            # --- 1. ä¿ç•™é›†åˆ†æ (å®Œå…¨ä¸å—å½±å“çš„ç”¨æˆ·) ---
            set_key, set_name = 'retain_set', 'ä¿ç•™é›† (è¯„ä¼°æ¨¡å‹é€šç”¨æ€§)'
            if set_key in original_results and set_key in unlearned_results and original_results[set_key]:
                logger.info(f"\nğŸ“‹ {set_name}æ€§èƒ½å¯¹æ¯”:")
                logger.info("-" * 60)
                orig = original_results[set_key]
                unlearned = unlearned_results[set_key]
                all_metrics_keys = sorted(list(set(orig.keys()) | set(unlearned.keys())))
                for metric_key in all_metrics_keys:
                    orig_val = orig.get(metric_key, 0.0)
                    unlearned_val = unlearned.get(metric_key, 0.0)
                    change = unlearned_val - orig_val
                    change_pct = (change / orig_val * 100) if orig_val != 0 else float('inf') if change > 0 else 0.0
                    indicator = "âœ… (æ€§èƒ½ä¿æŒè‰¯å¥½)" if abs(change_pct) < 10 else "âš ï¸ (æ€§èƒ½æ³¢åŠ¨è¾ƒå¤§)"
                    logger.info(f"  {metric_key:25s}: {orig_val:.4f} â†’ {unlearned_val:.4f} "
                                f"(Î”={change:+.4f}, {change_pct:+.1f}%) {indicator}")
            
            # --- 2. é—å¿˜é›†åˆ†æ (å—å½±å“çš„ç”¨æˆ·) ---
            set_key, set_name = 'forget_set', 'é—å¿˜é›† (è¯„ä¼°é—å¿˜æ•ˆæœå’Œè¿å¸¦æŸä¼¤)'
            if set_key in original_results and set_key in unlearned_results and original_results[set_key]:
                logger.info(f"\nğŸ“‹ {set_name}æ€§èƒ½å¯¹æ¯”:")
                
                orig = original_results[set_key]
                unlearned = unlearned_results[set_key]
                all_metrics_keys = sorted(list(set(orig.keys()) | set(unlearned.keys())))
                
                # [æ ¸å¿ƒä¿®æ”¹] å°†æŒ‡æ ‡åˆ†ä¸ºä¸¤ç»„æ‰“å°
                forgotten_metrics = [k for k in all_metrics_keys if "_forgotten" in k]
                retain_metrics = [k for k in all_metrics_keys if "_retain" in k]

                logger.info("\n  --- 1. é—å¿˜æ•ˆèƒ½ (Forget Efficacy) ---")
                logger.info("  (æ­¤éƒ¨åˆ†æ‰€æœ‰æŒ‡æ ‡è¶Šæ¥è¿‘0ï¼Œä¸‹é™å¹…åº¦è¶Šå¤§è¶Šå¥½)")
                logger.info("  " + "-" * 50)
                if forgotten_metrics:
                    for metric_key in forgotten_metrics:
                        orig_val = orig.get(metric_key, 0.0)
                        unlearned_val = unlearned.get(metric_key, 0.0)
                        change = unlearned_val - orig_val
                        change_pct = (change / orig_val * 100) if orig_val != 0 else -100.0 if unlearned_val == 0 else 0.0
                        indicator = "âœ… (é—å¿˜æˆåŠŸ)" if change < -0.05 or unlearned_val < 0.01 else "âŒ (é—å¿˜ä¸å½»åº•)"
                        logger.info(f"    {metric_key:23s}: {orig_val:.4f} â†’ {unlearned_val:.4f} "
                                    f"(Î”={change:+.4f}, {change_pct:+.1f}%) {indicator}")
                else:
                    logger.info("    æœªèƒ½è®¡ç®—é—å¿˜æ•ˆèƒ½æŒ‡æ ‡(æ•°æ®é‡‡æ ·é—®é¢˜)ã€‚")

                logger.info("\n  --- 2. è¿å¸¦æŸä¼¤ (Collateral Damage) ---")
                logger.info("  (æ­¤éƒ¨åˆ†æŒ‡æ ‡è¶Šç¨³å®šï¼Œæ³¢åŠ¨è¶Šå°è¶Šå¥½)")
                logger.info("  " + "-" * 50)
                if retain_metrics:
                    for metric_key in retain_metrics:
                        orig_val = orig.get(metric_key, 0.0)
                        unlearned_val = unlearned.get(metric_key, 0.0)
                        change = unlearned_val - orig_val
                        change_pct = (change / orig_val * 100) if orig_val != 0 else float('inf') if change > 0 else 0.0
                        indicator = "ğŸ‘ (å¯æ¥å—)" if abs(change_pct) < 40 else "ğŸ‘ (æŸä¼¤è¾ƒå¤§)"
                        logger.info(f"    {metric_key:23s}: {orig_val:.4f} â†’ {unlearned_val:.4f} "
                                    f"(Î”={change:+.4f}, {change_pct:+.1f}%) {indicator}")

    def evaluate_unlearning_effectiveness(self, original_results: Dict, unlearned_results: Dict):
        """è¯„ä¼°æ•´ä½“é—å¿˜æ•ˆæœ"""
        logger.info("\nğŸ¯ æ•´ä½“é—å¿˜æ•ˆæœè¯„ä¼°:")
        logger.info("-" * 60)
        
        forget_drops = []
        retain_drops = []
        
        # --- æ ¸å¿ƒä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å¸¦åç¼€çš„ metric key ---
        # ä½¿ç”¨é…ç½®ä¸­çš„ k_values æ¥è®¡ç®—ä¿ç•™/é—å¿˜æŒ‡æ ‡çš„å˜åŒ–ï¼Œé¿å…ç¡¬ç¼–ç å¹¶ä¿æŒä¸€è‡´æ€§
        for k in getattr(self, 'k_values', [10, 20]):
            retain_metric_key = f"hit@{k}_retain"
            # é—å¿˜æ•ˆèƒ½é€šå¸¸å…³æ³¨ Recall@Kï¼ˆå¸¦ _forgotten åç¼€ï¼‰
            forget_metric_key = f"recall@{k}_forgotten"

            if (retain_metric_key in original_results.get('retain_set', {}) and
                retain_metric_key in unlearned_results.get('retain_set', {})):
                orig_retain_val = original_results['retain_set'][retain_metric_key]
                unlearned_retain_val = unlearned_results['retain_set'][retain_metric_key]
                retain_drop = orig_retain_val - unlearned_retain_val
                retain_drops.append(retain_drop)

            if (forget_metric_key in original_results.get('forget_set', {}) and
                forget_metric_key in unlearned_results.get('forget_set', {})):
                orig_forget_val = original_results['forget_set'][forget_metric_key]
                unlearned_forget_val = unlearned_results['forget_set'][forget_metric_key]
                forget_drop = orig_forget_val - unlearned_forget_val
                forget_drops.append(forget_drop)
        
        if forget_drops and retain_drops:
            avg_forget_drop = sum(forget_drops) / len(forget_drops)
            avg_retain_drop = sum(retain_drops) / len(retain_drops)
            selectivity = avg_forget_drop - max(0, avg_retain_drop)
            
            logger.info(f"å¹³å‡é—å¿˜é›†æ€§èƒ½ä¸‹é™(è¶Šå¤§è¶Šå¥½): {avg_forget_drop:.4f}")
            logger.info(f"å¹³å‡ä¿ç•™é›†æ€§èƒ½ä¸‹é™(è¶Šå°è¶Šå¥½): {avg_retain_drop:.4f}")
            logger.info(f"é€‰æ‹©æ€§æŒ‡æ ‡ (Selectivity): {selectivity:.4f}")
            
            if selectivity > 0.1 and avg_retain_drop < 0.05:
                overall = "âœ… é—å¿˜æ•ˆæœä¼˜ç§€"
            elif selectivity > 0.05:
                overall = "âš ï¸ é—å¿˜æ•ˆæœè‰¯å¥½"
            else:
                overall = "âŒ é—å¿˜æ•ˆæœä¸€èˆ¬æˆ–è¾ƒå·®"
                
            logger.info(f"æ•´ä½“è¯„ä¼°: {overall}")
        
    def save_comparison_results(self, data_info: Dict, original_results: Dict, unlearned_results: Dict):
        """ä¿å­˜å¯¹æ¯”è¯„ä¼°ç»“æœ"""
        timestamp = int(time.time())
        results_file = f"results/comparison_evaluation_{timestamp}{self.output_suffix}.json"
        
        os.makedirs("results", exist_ok=True)
        
        # è®¡ç®—å˜åŒ–é‡
        changes = {}
        for set_key in ['all_users', 'retain_set', 'forget_set']:
            changes[set_key] = {}
            # åœ¨ --skip_all_users æƒ…å†µä¸‹ï¼Œéƒ¨åˆ†é›†åˆå¯èƒ½æœªè¯„ä¼°ï¼›æ­¤å¤„éœ€åˆ¤ç©º
            if set_key not in original_results or set_key not in unlearned_results:
                continue
            for metric in self.target_metrics:
                if metric in original_results[set_key] and metric in unlearned_results[set_key]:
                    orig_val = original_results[set_key][metric]
                    new_val = unlearned_results[set_key][metric]
                    changes[set_key][metric] = {
                        'absolute_change': new_val - orig_val,
                        'relative_change': ((new_val - orig_val) / orig_val * 100) if orig_val > 0 else 0
                    }
        
        # ä¿å­˜å®Œæ•´å¯¹æ¯”ç»“æœ
        comparison_results = {
            'evaluation_time': timestamp,
            'models': {
                'original_model': self.original_model_path,
                'unlearned_model': self.unlearned_model_path
            },
            'config': {
                'forget_ratio': self.forget_ratio,
                'eval_sample_size': self.eval_sample_size,
                'k_values': self.k_values,
                'target_metrics': self.target_metrics
            },
            'dataset_info': {
                'total_users': len(data_info['all_users']),
                'retain_users': len(data_info['retain_users']),
                'forget_users': len(data_info['forget_users']),
                'eval_retain_users': len(data_info['eval_retain_users']),
                'eval_forget_users': len(data_info['eval_forget_users'])
            },
            'performance_results': {
                'original': original_results,
                'unlearned': unlearned_results,
                'changes': changes
            }
        }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(comparison_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"\nğŸ“„ å¯¹æ¯”è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {results_file}")

    def analyze_performance(self):
        """åˆ†æä¸åŒæ•°æ®é›†çš„æ€§èƒ½å·®å¼‚"""
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š ä¸åŒæ•°æ®é›†æ€§èƒ½å¯¹æ¯”åˆ†æ")
        logger.info("=" * 80)
        all_users = self.results.get('all_users', {}) or {}
        retain_set = self.results.get('retain_set', {}) or {}
        forget_set = self.results.get('forget_set', {}) or {}

        logger.info("\nğŸ“‹ å„æ•°æ®é›†æ€§èƒ½æ€»ç»“:")
        logger.info("-" * 60)

        # Prepare table header
        ks = sorted(self.k_values)
        header_cols = ['metric'] + [f'all@{k}' for k in ks] + [f'retain@{k}' for k in ks] + [f'forget@{k}' for k in ks]
        logger.info("| %s |", " | ".join(header_cols))
        logger.info("|%s|", "|".join(['-' * (len(c)+2) for c in header_cols]))

        # For each metric type, print a row per metric (hit, ndcg, recall)
        metric_types = ['hit', 'ndcg', 'recall']
        for mtype in metric_types:
            for k in ks:
                key = f"{mtype}@{k}"
                a = all_users.get(key, None)
                r = retain_set.get(key, None)
                f = forget_set.get(key, None)
                a_s = f"{a:.4f}" if a is not None else "-"
                r_s = f"{r:.4f}" if r is not None else "-"
                f_s = f"{f:.4f}" if f is not None else "-"
                logger.info("| %s@%d | %s | %s | %s |", mtype, k, a_s, r_s, f_s)

    def save_results(self, data_info):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        timestamp = int(time.time())
        results_file = f"results/dataset_evaluation_{timestamp}{self.output_suffix}.json"
        
        os.makedirs("results", exist_ok=True)
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        full_results = {
            'evaluation_time': timestamp,
            'model_path': self.model_path,
            'config': {
                'forget_ratio': self.forget_ratio,
                'eval_sample_size': self.eval_sample_size,
                'k_values': self.k_values,
                'target_metrics': self.target_metrics
            },
            'dataset_info': {
                'total_users': len(data_info['all_users']),
                'retain_users': len(data_info['retain_users']),
                'forget_users': len(data_info['forget_users']),
                'eval_retain_users': len(data_info['eval_retain_users']),
                'eval_forget_users': len(data_info['eval_forget_users'])
            },
            'performance_results': self.results
        }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(full_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"\nğŸ“„ è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {results_file}")

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="P5æ¨èæ¨¡å‹æ•°æ®é›†åˆ’åˆ†è¯„ä¼°å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
    ä½¿ç”¨ç¤ºä¾‹:
    # è¯„ä¼°å•ä¸ªæ¨¡å‹
    python evaluate_datasets.py --model_path models/ML1M_sequential.pt
    
    # è¯„ä¼°é—å¿˜åæ¨¡å‹å¹¶æ·»åŠ åç¼€
    python evaluate_datasets.py --model_path models/ML1M_sequential_unlearned.pt --output_suffix _after_unlearning
    
    # å¯¹æ¯”åŸå§‹æ¨¡å‹å’Œé—å¿˜åæ¨¡å‹
    python evaluate_datasets.py --original_model models/original.pt --unlearned_model models/unlearned.pt --compare
    
    # è‡ªå®šä¹‰è¯„ä¼°å‚æ•°
    python evaluate_datasets.py --model_path models/model.pt --k_values 5,10,20,50 --eval_sample_size 100 --save_predictions
            """
    )
    
    # æ¨¡å‹é€‰æ‹©å‚æ•°ï¼ˆäº’æ–¥ç»„ï¼‰
    model_group = parser.add_mutually_exclusive_group(required=True)
    model_group.add_argument(
        '--model_path', 
        type=str,
        help='å•ä¸ªæ¨¡å‹æ–‡ä»¶è·¯å¾„'
    )
    model_group.add_argument(
        '--compare',
        action='store_true',
        help='å¯ç”¨å¯¹æ¯”æ¨¡å¼ï¼ˆéœ€è¦åŒæ—¶æŒ‡å®š --original_model å’Œ --unlearned_modelï¼‰'
    )
    
    # å¯¹æ¯”æ¨¡å¼å‚æ•°
    parser.add_argument(
        '--original_model',
        type=str,
        help='åŸå§‹æ¨¡å‹è·¯å¾„ï¼ˆå¯¹æ¯”æ¨¡å¼å¿…éœ€ï¼‰'
    )
    parser.add_argument(
        '--unlearned_model', 
        type=str,
        help='é—å¿˜åæ¨¡å‹è·¯å¾„ï¼ˆå¯¹æ¯”æ¨¡å¼å¿…éœ€ï¼‰'
    )
    
    # è¾“å‡ºé…ç½®
    parser.add_argument(
        '--output_suffix',
        type=str,
        default='',
        help='è¾“å‡ºæ–‡ä»¶ååç¼€'
    )
    
    # è¯„ä¼°å‚æ•°
    parser.add_argument(
        '--forget_ratio',
        type=float,
        default=0.01,
        help='é—å¿˜é›†æ¯”ä¾‹ (é»˜è®¤: 0.01)'
    )
    parser.add_argument(
        '--eval_sample_size',
        type=int,
        default=50,
        help='æ¯ä¸ªé›†åˆçš„è¯„ä¼°æ ·æœ¬å¤§å° (é»˜è®¤: 50)'
    )
    parser.add_argument(
        '--k_values',
        type=str,
        default='10,20',
        help='è¯„ä¼°çš„Kå€¼åˆ—è¡¨ï¼Œé€—å·åˆ†éš” (é»˜è®¤: 10,20)'
    )
    parser.add_argument(
        '--num_return_sequences',
        type=int,
        default=10,
        help='generate æ—¶æ¯ä¸ªè¾“å…¥è¿”å›çš„åºåˆ—æ•°ï¼ˆé»˜è®¤: 10ï¼‰'
    )
    parser.add_argument(
        '--num_beams',
        type=int,
        default=10,
        help='generate æ—¶ä½¿ç”¨çš„ beam å¤§å°ï¼ˆé»˜è®¤: 10ï¼‰'
    )
    parser.add_argument(
        '--max_gen_len',
        type=int,
        default=150,
        help='generate æ—¶çš„ max_lengthï¼ˆé»˜è®¤: 150ï¼‰'
    )
    parser.add_argument(
        '--eval_rescan_mode',
        type=str,
        choices=['auto', 'skip', 'force'],
        default='auto',
        help='è¯„ä¼°å‰æ˜¯å¦é‡æ–°æ‰«æ Î”act é˜ˆå€¼: auto=éµå¾ªæ¨¡å‹é…ç½®, skip=ç¦ç”¨, force=å¼ºåˆ¶å¯ç”¨'
    )
    # æ¨ç†é˜¶æ®µæ¸©åº¦æ ‡å®šä¸ç½®ä¿¡å›é€€
    parser.add_argument(
        '--base_temperature',
        type=float,
        default=1.0,
        help='ä¸»è®°å¿†/åŸºç¡€æ¨¡å‹ç”Ÿæˆæ¸©åº¦ (å¯¹beam searchçš„logitsç¼©æ”¾)'
    )
    parser.add_argument(
        '--side_temperature',
        type=float,
        default=1.2,
        help='ä¾§è®°å¿†ç”Ÿæˆæ¸©åº¦ (>1 ä»¥è½¯åŒ–è¿‡åº¦è‡ªä¿¡)'
    )
    parser.add_argument(
        '--use_entropy_fallback',
        action='store_true',
        help='å¯ç”¨åŸºäºå½’ä¸€åŒ–ç†µçš„ç½®ä¿¡å›é€€ (å½“ä¸ç¡®å®šæ€§é«˜æ—¶å›é€€ä¸»è®°å¿†)'
    )
    parser.add_argument(
        '--conf_fallback_threshold',
        type=float,
        default=0.85,
        help='è§¦å‘å›é€€çš„å½’ä¸€åŒ–ç†µé˜ˆå€¼ (0-1, è¶Šå¤§è¶Šä¸ç¡®å®š)'
    )
    parser.add_argument(
        '--min_unique_ratio',
        type=float,
        default=0.3,
        help='è‹¥å»é‡åçš„æ¨èæ•°/TopK å°äºæ­¤æ¯”ç‡ï¼Œåˆ™è§¦å‘å›é€€åˆ°ä¸»è®°å¿†'
    )
    # Certified Forgetful Decoding (CFD)
    parser.add_argument(
        '--certified_forgetful_decoding',
        action='store_true',
        help='å¯ç”¨è§£ç æŠ¤æ ï¼šåœ¨ä¾§è®°å¿†å¯¹é—å¿˜ç”¨æˆ·ç”Ÿæˆæ—¶ï¼Œç¦æ­¢è¾“å‡ºè¢«é—å¿˜æ¡ç›®ï¼ˆbad_words_ids + penaltyï¼‰'
    )
    parser.add_argument(
        '--forbidden_penalty',
        type=float,
        default=20.0,
        help='å¯¹è¢«ç¦æ­¢tokenæ–½åŠ çš„å¯¹æ•°å‡ ç‡æƒ©ç½šï¼Œæ•°å€¼è¶Šå¤§è¶Šéš¾è¢«é€‰ä¸­ï¼ˆé»˜è®¤: 20.0ï¼‰'
    )
    parser.add_argument(
        '--emit_certificates',
        action='store_true',
        help='å¯ç”¨åä¼šåœ¨ results/certificates/ ç›®å½•ä¸‹ä¸ºæ¯ä¸ªé—å¿˜ç”¨æˆ·ç”Ÿæˆä¸€æ¬¡æ€§è¯ä¹¦ï¼Œè®°å½•æ˜¯å¦å‡ºç°è¿è§„æ¨è'
    )
    
    # å…¶ä»–é€‰é¡¹
    parser.add_argument(
        '--save_predictions',
        action='store_true',
        help='ä¿å­˜è¯¦ç»†çš„æ¨èç»“æœ'
    )
    parser.add_argument(
        '--save_examples_num',
        type=int,
        default=10,
        help='ä¿å­˜ç”¨äºäººå·¥æŸ¥çœ‹çš„ç¤ºä¾‹ç”¨æˆ·æ•°é‡ï¼ˆé»˜è®¤: 10ï¼‰'
    )
    parser.add_argument(
        '--skip_all_users',
        action='store_true',
        help='è·³è¿‡å…¨ä½“ç”¨æˆ·è¯„ä¼°ï¼ŒåŠ é€Ÿè¿è¡Œï¼ˆä»…è¯„ä¼°ä¿ç•™é›†ä¸é—å¿˜é›†ï¼‰'
    )
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—'
    )

    parser.add_argument(
        '--disable_fallback',
        action='store_true',
        help='ç¦ç”¨åœ¨ç”Ÿæˆç»“æœä¸è¶³æ—¶ç”¨æµè¡Œç‰©å“è¡¥é½çš„å›é€€ç­–ç•¥'
    )

    parser.add_argument(
        '--dual_memory_artifacts',
        type=str,
        default=None,
        help='Dual-memory ç»„åˆæ¨¡å‹çš„artifactè·¯å¾„ (åŒ…å«ä¾§è®°å¿†ä¸è·¯ç”±å™¨)'
    )
    parser.add_argument(
        '--dual_memory_threshold',
        type=float,
        default=None,
        help='åœ¨åŠ è½½dual-memory artifactæ—¶è¦†ç›–é»˜è®¤è·¯ç”±é˜ˆå€¼'
    )
    
    args = parser.parse_args()
    
    # éªŒè¯å¯¹æ¯”æ¨¡å¼å‚æ•°
    if args.compare:
        if not args.original_model or not args.unlearned_model:
            parser.error("å¯¹æ¯”æ¨¡å¼éœ€è¦åŒæ—¶æŒ‡å®š --original_model å’Œ --unlearned_model")
        if not os.path.exists(args.original_model):
            parser.error(f"åŸå§‹æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.original_model}")
        if not os.path.exists(args.unlearned_model):
            parser.error(f"é—å¿˜åæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.unlearned_model}")
    elif args.model_path:
        if not os.path.exists(args.model_path):
            parser.error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model_path}")
    
    return args

def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    logger.info("P5 æ¨èæ¨¡å‹è¯„ä¼°å·¥å…·å¯åŠ¨")
    if args.compare:
        logger.info("è¯„ä¼°æ¨¡å¼ï¼šå¯¹æ¯”è¯„ä¼°")
        logger.info(f"åŸå§‹æ¨¡å‹: {args.original_model}")
        logger.info(f"é—å¿˜åæ¨¡å‹: {args.unlearned_model}")
    else:
        logger.info("è¯„ä¼°æ¨¡å¼ï¼šå•æ¨¡å‹è¯„ä¼°")
        logger.info(f"æ¨¡å‹è·¯å¾„: {args.model_path}")

    logger.info(f"è¯„ä¼°å‚æ•°: forget_ratio={args.forget_ratio}, eval_sample_size={args.eval_sample_size}, k_values={args.k_values}")
    logger.info(f"eval_rescan_mode: {args.eval_rescan_mode}")
    
    # åˆ›å»ºè¯„ä¼°å™¨å¹¶è¿è¡Œ
    evaluator = DatasetEvaluator(args)
    success = evaluator.run_evaluation()
    
    if success:
        logger.info("âœ… è¯„ä¼°æˆåŠŸå®Œæˆ")
    else:
        logger.error("âŒ è¯„ä¼°å¤±è´¥")
        sys.exit(1)

if __name__ == "__main__":
    main()