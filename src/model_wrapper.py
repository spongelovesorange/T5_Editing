# File: model_wrapper.py (Corrected Version)

#!/usr/bin/env python3
"""
P5æ¨èæ¨¡å‹åŒ…è£…å™¨ - æœ€ç»ˆä¿®å¤ç‰ˆ
"""

import os
import torch
import torch.nn as nn
import logging
import re
from typing import List, Dict, Any, Optional

# å»¶è¿Ÿå¯¼å…¥transformersï¼Œä¾¿äºåœ¨ç¼ºå¤±æ—¶ç»™å‡ºæ¸…æ™°æç¤º
try:
    from transformers import T5ForConditionalGeneration, T5Tokenizer, T5Config
except ImportError as e:
    T5ForConditionalGeneration = T5Tokenizer = T5Config = None
    _transformers_import_error = e
else:
    _transformers_import_error = None

class P5ModelWrapper:
    """P5æ¨èæ¨¡å‹çš„åŒ…è£…å™¨ç±»ï¼ˆæœ€ç»ˆä¿®å¤ç‰ˆï¼Œæ”¯æŒæ™ºèƒ½åŠ è½½ï¼‰"""
    
    def __init__(self, model_path: str, device: str = 'cuda', t5_local_dir: Optional[str] = None, checkpoint: Optional[Dict] = None):
        """
        åˆå§‹åŒ–P5æ¨¡å‹åŒ…è£…å™¨
        Args:
            model_path: P5æ¨¡å‹æ ‡è¯†ç¬¦æˆ–æƒé‡æ–‡ä»¶è·¯å¾„ï¼ˆä¸»è¦ä¾›æ—¥å¿—è®°å½•ï¼‰
            device: è®¾å¤‡ç±»å‹ ('cuda' æˆ– 'cpu')
            t5_local_dir: æœ¬åœ°t5æ¨¡å‹ç›®å½•
            checkpoint: (æ–°å¢) å¦‚æœæä¾›ï¼Œåˆ™ç›´æ¥ä»æ­¤checkpointåŠ è½½ï¼Œè€Œä¸æ˜¯ä»model_pathåŠ è½½æ–‡ä»¶
        """
        self.model_path = model_path
        self.device = device
        self.checkpoint = checkpoint
        self.logger = logging.getLogger(__name__)

        self.logger.info("ğŸ” æ¨¡å‹åˆå§‹åŒ–: æ£€æŸ¥ä¾èµ–åº“ transformers / sentencepiece ...")
        if _transformers_import_error is not None:
            raise _transformers_import_error
        if t5_local_dir and os.path.isdir(t5_local_dir):
            self.t5_source = t5_local_dir
        else:
            # 2. æ£€æŸ¥ç¯å¢ƒå˜é‡
            env_path = os.environ.get('P5_T5_MODEL_DIR')
            if env_path and os.path.isdir(env_path):
                self.t5_source = env_path
            else:
                # 3. è‡ªåŠ¨è®¡ç®—é¡¹ç›®å†…çš„é»˜è®¤è·¯å¾„
                try:
                    # PROJECT_ROOT åº”è¯¥æ˜¯ train_dual_memory.py æ‰€åœ¨çš„ç›®å½•
                    # train_dual_memory.py -> src/ -> model_wrapper.py
                    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
                    default_local_path = os.path.join(project_root, "hf_models", "t5-small")
                except NameError:
                    # å¦‚æœ __file__ ä¸å¯ç”¨ï¼Œåˆ™ä»å½“å‰å·¥ä½œç›®å½•çŒœæµ‹
                    default_local_path = os.path.join(os.getcwd(), "hf_models", "t5-small")

                if os.path.isdir(default_local_path):
                    self.t5_source = default_local_path
                else:
                    # 4. å¦‚æœä»¥ä¸Šå…¨éƒ¨å¤±è´¥ï¼Œæ‰å›é€€åˆ°åœ¨çº¿ä¸‹è½½ï¼ˆåœ¨æ‚¨çš„æƒ…å†µä¸‹ä¼šæŠ¥é”™ï¼‰
                    self.t5_source = 't5-small'
                    self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æœ¬åœ°T5æ¨¡å‹è·¯å¾„ï¼Œå°†å°è¯•åœ¨çº¿ä¸‹è½½ '{self.t5_source}'")

        self.logger.info("ğŸ“¦ åŠ è½½T5åˆ†è¯å™¨: %s", self.t5_source)
        self.tokenizer = T5Tokenizer.from_pretrained(self.t5_source)
        
        self.model = None
        self._load_model()
    
    def _load_model(self):
        """æ™ºèƒ½åŠ è½½P5æ¨¡å‹ï¼Œå…¼å®¹æ–°æ—§ä¸¤ç§checkpointæ ¼å¼ï¼Œå¹¶æ”¯æŒDual-memory artifactçš„æ¢å¤

        é€»è¾‘æ¥æº: evaluate_datasets.initialize_modelï¼ˆå·²è¿ç§»å¹¶é€‚é…åˆ° wrapperï¼‰ã€‚
        """
        if self.checkpoint is None and os.path.isdir(self.model_path):
                    try:
                        self.logger.info(f"ğŸ¯ æ£€æµ‹åˆ°HuggingFaceæ¨¡å‹ç›®å½•ï¼Œå°è¯•ä»ç›®å½•åŠ è½½: {self.model_path}")
                        self.logger.info(f"ğŸ” é‡æ–°åŠ è½½ Tokenizer ä»¥ç¡®ä¿åŒ¹é…: {self.model_path}")
                        self.tokenizer = T5Tokenizer.from_pretrained(self.model_path)
                        # ä½¿ç”¨ from_pretrained åŠ è½½æ¨¡å‹ï¼Œåˆ†è¯å™¨å·²åœ¨ __init__ ä¸­åŠ è½½
                        model = T5ForConditionalGeneration.from_pretrained(self.model_path).to(self.device)
                        
                        # æ£€æŸ¥å¹¶åŒæ­¥è¯è¡¨å¤§å° (ä¸ evaluate_datasets.py ä¸­çš„é€»è¾‘ä¸€è‡´)
                        tok_size = len(self.tokenizer)
                        emb_size = model.get_input_embeddings().weight.size(0)

                        # [ å…³é”® ] ç§»é™¤é‚£ä¸ªé”™è¯¯çš„ resize é€»è¾‘ (HF from_pretrained åº”è¯¥å·²ç»å¤„ç†äº†)
                        # æˆ‘ä»¬åªåœ¨ tok_size > emb_size æ—¶æ‰éœ€è¦ä»‹å…¥
                        if tok_size > emb_size:
                            self.logger.warning(f"âš ï¸ Tokenizer è¯è¡¨å¤§å° ({tok_size}) å¤§äº æ¨¡å‹ ({emb_size}). æ­£åœ¨è°ƒæ•´æ¨¡å‹å¤§å°...")
                            model.resize_token_embeddings(tok_size)
                        elif emb_size > tok_size:
                            self.logger.warning(f"âš ï¸ æ¨¡å‹è¯è¡¨å¤§å° ({emb_size}) å¤§äº Tokenizer ({tok_size}). è¿™å¯èƒ½æ˜¯ä¸€ä¸ªé…ç½®é”™è¯¯ï¼Œä½†æˆ‘ä»¬å°†ç»§ç»­...")

                        self.model = model
                        self.model.eval()
                        self.logger.info(f"âœ… HuggingFace æ¨¡å‹åŠ è½½å®Œæˆ, device={self.device}")
                        
                        # [ å…³é”® ] åŠ è½½æˆåŠŸåå¿…é¡»ç«‹å³è¿”å›ï¼Œè·³è¿‡åç»­çš„ torch.load é€»è¾‘
                        return 
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ æ— æ³•å°†ç›®å½•ä½œä¸ºHuggingFaceæ¨¡å‹åŠ è½½, å°†å›é€€åˆ° checkpoint åŠ è½½... Error: {e}")

        if self.checkpoint is None:
            self.logger.info(f"ğŸ¯ ä»æ–‡ä»¶åŠ è½½P5æ¨¡å‹æƒé‡: {self.model_path}")
            if not os.path.exists(self.model_path):
                self.logger.warning(f"âš ï¸ æ¨¡å‹æƒé‡æ–‡ä»¶ {self.model_path} ä¸å­˜åœ¨ï¼Œå°†ä»…ä½¿ç”¨é¢„è®­ç»ƒ {self.t5_source} åŸºç¡€æ¨¡å‹")
                self.model = T5ForConditionalGeneration.from_pretrained(self.t5_source).to(self.device)
                self.model.eval()
                return
            # PyTorch 2.6 é»˜è®¤ weights_only=True ä¼šå¯¼è‡´è€å¼checkpointæŠ¥ _pickle.UnpicklingError
            # è¿™é‡Œæ˜¾å¼è®¾ç½®ä¸º Falseï¼ˆå‰ææ˜¯æ¥è‡ªå¯ä¿¡æ¥æºçš„æœ¬åœ°æ–‡ä»¶ï¼‰
            self.checkpoint = torch.load(self.model_path, map_location=self.device, weights_only=False)
        else:
            self.logger.info(f"ğŸ¯ ä½¿ç”¨ä¼ å…¥ checkpoint åŠ è½½æ¨¡å‹: {self.model_path}")

        checkpoint = self.checkpoint

        # small helpers copied/adapted from evaluate_datasets
        def _ensure_special_tokens(tokenizer_obj):
            if tokenizer_obj is None or not hasattr(tokenizer_obj, 'add_special_tokens'):
                return 0
            forget_id = tokenizer_obj.convert_tokens_to_ids('<forget>')
            retain_id = tokenizer_obj.convert_tokens_to_ids('<retain>')
            unk_id = getattr(tokenizer_obj, 'unk_token_id', None)
            tokens_to_add = []
            if forget_id is None or (unk_id is not None and forget_id == unk_id):
                tokens_to_add.append('<forget>')
            if retain_id is None or (unk_id is not None and retain_id == unk_id):
                tokens_to_add.append('<retain>')
            if not tokens_to_add:
                return 0
            special_tokens_dict = {'additional_special_tokens': tokens_to_add}
            try:
                added = tokenizer_obj.add_special_tokens(special_tokens_dict)
                if added > 0:
                    self.logger.info("ğŸ” è¡¥å……ç‰¹æ®ŠToken: æ–°å¢ %d ä¸ª (%s)", added, ','.join(tokens_to_add))
                return int(added)
            except Exception as tok_err:
                self.logger.warning("æ·»åŠ ç‰¹æ®ŠTokenå¤±è´¥: %s", tok_err)
                return 0

        def _sync_vocab_size(model_obj, tokenizer_obj, min_expand: int = 0):
            if tokenizer_obj is None or model_obj is None:
                return 0
            desired_size = len(tokenizer_obj)
            if desired_size <= 0:
                return 0
            current_size = model_obj.get_input_embeddings().weight.size(0)
            if desired_size <= current_size and min_expand <= 0:
                return 0
            target_size = max(desired_size, current_size + max(min_expand, 0))
            if target_size == current_size:
                return 0
            model_obj.resize_token_embeddings(target_size)
            with torch.no_grad():
                embeddings = model_obj.get_input_embeddings().weight
                ref_slice = embeddings[:current_size] if current_size > 0 else None
                if ref_slice is not None and ref_slice.numel() > 0 and target_size > current_size:
                    mu = ref_slice.mean(dim=0)
                    sigma = ref_slice.std(dim=0)
                    sigma = sigma.clamp(min=1e-6)
                    embeddings[current_size:target_size] = mu + torch.randn_like(embeddings[current_size:target_size]) * sigma * 0.01
            self.logger.info("ğŸ” è¯è¡¨åŒæ­¥: æ¨¡å‹vocab=%d -> %d", current_size, target_size)
            return target_size - current_size

        def _align_lora_output_dims(editor: Optional[Any]) -> None:
            if editor is None:
                return
            for layer_name, module in editor.side_modules.items():
                if not hasattr(module, 'lora_B') or module.lora_B is None:
                    continue
                target_out = module.weight.shape[0]
                current_out = module.lora_B.shape[0]
                if current_out == target_out:
                    continue
                old_param = module.lora_B.data
                new_param = old_param.new_zeros((target_out, old_param.shape[1]))
                rows_copy = min(target_out, current_out)
                if rows_copy > 0:
                    new_param[:rows_copy] = old_param[:rows_copy]
                module.lora_B = nn.Parameter(new_param)
                self.logger.info("LoRAå±‚è¾“å‡ºç»´åº¦è°ƒæ•´: %s (%d -> %d)", layer_name, current_out, target_out)

        # Detect format and infer vocab size
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            state_dict_for_size_check = checkpoint['model_state_dict']
        else:
            state_dict_for_size_check = checkpoint

        vocab_size = None
        if isinstance(state_dict_for_size_check, dict) and 'shared.weight' in state_dict_for_size_check:
            try:
                vocab_size = int(state_dict_for_size_check['shared.weight'].shape[0])
                self.logger.info(f"ä»checkpointæ¨æ–­åˆ° vocab_size={vocab_size}")
            except Exception:
                vocab_size = None

        if vocab_size is None:
            vocab_size = None

        # Build base model with correct vocab if possible
        try:
            if vocab_size is not None:
                cfg = T5Config.from_pretrained(self.t5_source, vocab_size=vocab_size)
                base_model = T5ForConditionalGeneration(cfg)
            else:
                base_model = T5ForConditionalGeneration.from_pretrained(self.t5_source)
        except Exception:
            self.logger.warning("æ— æ³•ä½¿ç”¨æœ¬åœ°/è¿œç¨‹T5é…ç½®ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨é»˜è®¤T5-smallå®ä¾‹åŒ–")
            base_model = T5ForConditionalGeneration.from_pretrained(self.t5_source)

        base_model = base_model.to(self.device)

        # If this is a dual-memory artifact (contains router_classifier_state_dict), perform WISE editor wrapping and LoRA application
        if isinstance(checkpoint, dict) and 'router_classifier_state_dict' in checkpoint:
            self.logger.info("æ£€æµ‹åˆ° Dual-memory artifactï¼Œæ¢å¤ä¸»è®°å¿†ä¸ä¾§è®°å¿†å¢é‡...")
            wise_config = checkpoint.get('wise_config', {})

            # Ensure tokenizer has special tokens
            added = _ensure_special_tokens(self.tokenizer)

            # Create WISE editor from dual_memory module if available
            try:
                from src.dual_memory import WISEUnlearningEditor as RealWISEEditor
                wise_editor = RealWISEEditor(base_model, self.tokenizer, wise_config)
            except Exception:
                self.logger.info("WISEUnlearningEditor æœªåœ¨ src.dual_memory ä¸­æ‰¾åˆ°ï¼Œä½¿ç”¨æœ¬åœ°å ä½å®ç°")
                wise_editor = None

            # Load main weights
            try:
                base_model.load_state_dict(checkpoint['model_state_dict'], strict=False)
                self.logger.info("âœ… ä¸»è®°å¿†æƒé‡åŠ è½½æˆåŠŸ")
            except Exception as e:
                self.logger.warning(f"åŠ è½½ä¸»è®°å¿†æƒé‡æ—¶å‡ºç°é—®é¢˜: {e}")

            try:
                _sync_vocab_size(base_model, self.tokenizer, min_expand=added)
                _align_lora_output_dims(wise_editor)
            except Exception as exc:
                self.logger.warning(f"è¯è¡¨/LoRA è°ƒæ•´å¤±è´¥: {exc}")

            # Load router classifier if present
            router_sd = checkpoint.get('router_classifier_state_dict', None)
            if router_sd and wise_editor is not None and getattr(wise_editor, 'router_classifier', None) is not None:
                try:
                    wise_editor.router_classifier.load_state_dict(router_sd)
                    wise_editor.router_classifier.eval()
                    self.logger.info("âœ… è·¯ç”±åˆ†ç±»å™¨åŠ è½½æˆåŠŸ")
                except Exception as _e:
                    self.logger.warning(f"è·¯ç”±åˆ†ç±»å™¨æƒé‡åŠ è½½å¤±è´¥ï¼Œå°†å›é€€åˆ° Î”act-only è·¯ç”±: {_e}")

            # Apply side deltas (LoRA or old-style)
            applied_side = False
            if 'lora_side_deltas' in checkpoint:
                lora_deltas = checkpoint['lora_side_deltas'] or {}
                loaded = 0

                def _copy_with_resize(target: torch.Tensor, source: torch.Tensor, tensor_name: str) -> None:
                    src_tensor = source.to(target.device)
                    if target.shape == src_tensor.shape:
                        target.copy_(src_tensor)
                        return
                    if target.dim() != src_tensor.dim():
                        raise RuntimeError(f"LoRA tensorç»´åº¦ä¸åŒ¹é…[{tensor_name}]: target={target.shape}, source={src_tensor.shape}")
                    min_shape = tuple(min(t, s) for t, s in zip(target.shape, src_tensor.shape))
                    slices = tuple(slice(0, ms) for ms in min_shape)
                    target[slices] = src_tensor[slices]

                if wise_editor is not None:
                    for name, comp in lora_deltas.items():
                        if name in wise_editor.side_modules:
                            module = wise_editor.side_modules[name]
                            if hasattr(module, 'lora_A') and hasattr(module, 'lora_B'):
                                with torch.no_grad():
                                    if 'lora_A' in comp:
                                        _copy_with_resize(module.lora_A, comp['lora_A'], f"{name}.lora_A")
                                    if 'lora_B' in comp:
                                        _copy_with_resize(module.lora_B, comp['lora_B'], f"{name}.lora_B")
                                    if 'scaling' in comp:
                                        module.scaling = comp['scaling']
                                loaded += 1
                    self.logger.info(f"âœ… LoRA ä¾§è®°å¿†å¢é‡åŠ è½½å®Œæˆ: {loaded}/{len(lora_deltas)}")
                    applied_side = True

            elif 'side_weight_deltas' in checkpoint and wise_editor is not None:
                for name, delta in checkpoint['side_weight_deltas'].items():
                    if name in wise_editor.side_modules:
                        module = wise_editor.side_modules[name]
                        if hasattr(module, 'side_weight'):
                            with torch.no_grad():
                                module.side_weight.add_(delta.to(module.side_weight.device))
                self.logger.info("âœ… ä¾§è®°å¿†(æ—§æ ¼å¼)åº”ç”¨æˆåŠŸ")
                applied_side = True

            # Restore thresholds and router feature norms if present
            if 'epsilon_threshold' in checkpoint and wise_editor is not None:
                wise_editor.epsilon_threshold = checkpoint.get('epsilon_threshold', wise_editor.epsilon_threshold)
            if 'router_prob_threshold' in checkpoint and checkpoint['router_prob_threshold'] is not None and wise_editor is not None:
                try:
                    wise_editor.router_prob_threshold = float(checkpoint['router_prob_threshold'])
                except Exception:
                    pass

            rfn = checkpoint.get('router_feature_norm', None)
            if rfn and wise_editor is not None:
                try:
                    mean = rfn.get('mean', None)
                    std = rfn.get('std', None)
                    eps_ref = rfn.get('epsilon_ref', None)
                    if mean is not None and std is not None:
                        mean_t = mean.to(self.device) if hasattr(mean, 'to') else torch.tensor(mean, device=self.device, dtype=torch.float32)
                        std_t = std.to(self.device) if hasattr(std, 'to') else torch.tensor(std, device=self.device, dtype=torch.float32)
                        wise_editor.router_feature_dataset = {'mean': mean_t, 'std': std_t, 'epsilon_ref': float(eps_ref) if eps_ref is not None else None}
                        if 'input_dim' in rfn:
                            wise_editor.router_input_dim = int(rfn['input_dim'])
                        if 'feature_mode' in rfn and rfn['feature_mode']:
                            wise_editor.router_feature_mode = str(rfn['feature_mode']).lower()
                        self.logger.info("âœ… æ¢å¤è·¯ç”±ç‰¹å¾å½’ä¸€åŒ–ä¿¡æ¯")
                except Exception as _e:
                    self.logger.warning(f"æ¢å¤è·¯ç”±ç‰¹å¾å½’ä¸€åŒ–å¤±è´¥: {_e}")

            # attach editor ref if present
            if wise_editor is not None:
                setattr(base_model, 'wise_editor_ref', wise_editor)

        else:
            # Standard model checkpoint path
            self.logger.info("æ£€æµ‹åˆ°æ ‡å‡†æ¨¡å‹ checkpointï¼Œæ‰§è¡Œæ ‡å‡†åŠ è½½æµç¨‹...")
            state_dict_to_load = checkpoint.get('model_state_dict', checkpoint) if isinstance(checkpoint, dict) else checkpoint
            try:
                base_model.load_state_dict(state_dict_to_load, strict=False)
            except Exception as e:
                self.logger.warning(f"åŠ è½½æ ‡å‡†æ¨¡å‹æƒé‡æ—¶å‡ºç°é—®é¢˜: {e}")
            added = _ensure_special_tokens(self.tokenizer)
            try:
                _sync_vocab_size(base_model, self.tokenizer, min_expand=added)
                _align_lora_output_dims(None)
            except Exception as emb_err:
                self.logger.warning(f"åŸå§‹æ¨¡å‹æ‰©å±•ç‰¹æ®ŠTokenåµŒå…¥å¤±è´¥: {emb_err}")

        # finalize
        self.model = base_model.to(self.device)
        self.model.eval()

        # sync editor devices if present
        wise_editor_sync = getattr(self.model, 'wise_editor_ref', None)
        if wise_editor_sync is not None:
            try:
                target_device = next(self.model.parameters()).device
                wise_editor_sync.device = target_device
                if getattr(wise_editor_sync, 'router_classifier', None) is not None:
                    wise_editor_sync.router_classifier.to(target_device)
            except Exception as _e:
                self.logger.warning(f"WISEç¼–è¾‘å™¨è®¾å¤‡åŒæ­¥è­¦å‘Š: {_e}")

        # record some metadata
        params = sum(p.numel() for p in self.model.parameters())
        self.logger.info(f"æ¨¡å‹åŠ è½½/æ¢å¤å®Œæˆï¼Œdevice={self.device}, params={params}")

    def get_model(self):
        return self.model
    
    def get_tokenizer(self):
        return self.tokenizer

    def load_from_checkpoint(self, checkpoint_path: str, local_t5_dir: Optional[str] = None) -> None:
        """
        Load model weights from a checkpoint file (new or old format). This wraps the existing
        `_load_model` behavior but allows specifying a different checkpoint without re-creating
        the wrapper.
        """
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
        # åŒä¸Šï¼Œå…¼å®¹ PyTorch 2.6 çš„é»˜è®¤è¡Œä¸ºå˜æ›´
        self.checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
        # re-run the loader logic
        self._load_model()

    def load_dual_runtime(self, artifact_path: str, device: Optional[str] = None, threshold: Optional[float] = None):
        """
        Load a DualMemory runtime from artifacts and return DualMemoryRuntime instance.
        This delegates to src.dual_memory.load_dual_memory_runtime to keep logic colocated.
        """
        try:
            from src.dual_memory import load_dual_memory_runtime
        except Exception as e:
            raise RuntimeError("dual_memory utilities unavailable: %s" % e)
        runtime = load_dual_memory_runtime(artifact_path, device=device or self.device, threshold=threshold)
        return runtime

    def get_dual_runtime(self, artifact_path: str, device: Optional[str] = None, threshold: Optional[float] = None):
        """Convenience alias for load_dual_runtime."""
        return self.load_dual_runtime(artifact_path, device=device, threshold=threshold)

    @classmethod
    def from_dual_memory_artifacts(cls, artifact_path: str, device: Optional[str] = None, threshold_override: Optional[float] = None):
        """Construct a P5ModelWrapper directly from dual-memory artifacts.

        This will load the DualMemoryRuntime (model + tokenizer + adapter) and
        attach it to the wrapper instance as `dual_runtime` for convenient access.
        """
        try:
            from src.dual_memory import load_dual_memory_runtime
        except Exception as e:
            raise RuntimeError(f"Unable to import dual_memory utilities: {e}")

        device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        runtime = load_dual_memory_runtime(artifact_path, device=device, threshold=threshold_override)

        # Create a minimal wrapper and attach runtime
        wrapper = cls(model_path=artifact_path, device=device)
        wrapper.model = runtime.model
        wrapper.tokenizer = runtime.tokenizer
        wrapper.dual_runtime = runtime
        return wrapper

    def _extract_items_from_text(self, text: str) -> List[str]:
        """ä»ç”Ÿæˆçš„æ–‡æœ¬ä¸­æå–ç‰©å“ID"""
        item_pattern = re.findall(r'item[_\s]*(\d+)', text, re.IGNORECASE)
        return item_pattern
    
    # ä¿®å¤ï¼šè¯¥å‡½æ•°ä¸å†ä½¿ç”¨ï¼Œå…¶é€»è¾‘å·²ç§»è‡³ evaluate_datasets.py
    # ä¿ç•™æ­¤å‡½æ•°æ˜¯ä¸ºäº†ç¡®ä¿ P5ModelWrapper çš„å®Œæ•´æ€§ï¼Œä½†å®é™…è°ƒç”¨å·²æ›´æ”¹
    def generate_simple_recommendation(self, prompt: str, max_items: int = 20) -> List[str]:
        """ç®€åŒ–çš„æ¨èç”Ÿæˆæ–¹æ³•ï¼ˆå·²åºŸå¼ƒï¼‰"""
        self.logger.warning("è¯¥å‡½æ•°å·²åºŸå¼ƒï¼Œå…¶é€»è¾‘å·²ç§»è‡³ evaluate_datasets.py ä»¥ç¡®ä¿ WISE è·¯ç”±å’Œç”Ÿæˆæµç¨‹çš„åŸå­æ€§ã€‚")
        return []

    def predict_router_output(self, prompt: str) -> float:
            """
            [CIU å…³é”®ä¿®å¤ç‰ˆ]
            é¢„æµ‹è·¯ç”±å™¨çš„Sigmoidè¾“å‡ºå€¼ã€‚ç¡®ä¿å‰å‘ä¼ æ’­æ–¹å¼ä¸è®­ç»ƒæ—¶ä¸€è‡´ã€‚
            """
            wise_editor = getattr(self.model, 'wise_editor_ref', None)
            if not wise_editor:
                return 0.0

            self.model.eval()
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
            
            # [å…³é”®ä¿®å¤] æ„é€ ä¸€ä¸ªä¸è®­ç»ƒæ—¶ç»“æ„ç›¸åŒçš„è¾“å…¥å­—å…¸
            dummy_labels = torch.full_like(inputs['input_ids'], self.tokenizer.pad_token_id)
            inputs['labels'] = dummy_labels

            def capture_activations(use_side: bool):
                wise_editor.set_routing_state(use_side)
                wise_editor.captured_activations.clear()
                with torch.no_grad():
                    _ = self.model(**inputs)
                outputs = []
                for name in wise_editor.router_target_layers:
                    if name in wise_editor.captured_activations:
                        outputs.append(wise_editor.captured_activations[name][:, 0, :])
                if not outputs:
                    return None
                if len(outputs) == 1:
                    return outputs[0]
                return torch.stack(outputs, dim=0).mean(dim=0)

            main_act = capture_activations(False)
            side_act = capture_activations(True)
            wise_editor.set_routing_state(False)

            if main_act is None or side_act is None:
                return 0.0

            delta_norm = (side_act - main_act).norm(p=2, dim=-1, keepdim=True)
            epsilon = wise_editor.epsilon_threshold or 1.0

            if wise_editor.router_feature_mode in ("delta-only", "delta_only"):
                feature_vec = torch.cat([delta_norm, delta_norm - epsilon], dim=-1)
            else:
                main_mean = main_act.mean(dim=-1, keepdim=True)
                feature_vec = torch.cat([delta_norm, main_mean], dim=-1)

            dataset_info = getattr(wise_editor, 'router_feature_dataset', None)
            if dataset_info is not None:
                mean = dataset_info['mean'].to(feature_vec.device)
                std = dataset_info['std'].to(feature_vec.device)
                feature_vec = (feature_vec - mean) / std.clamp(min=1e-6)

            try:
                router_device = next(wise_editor.router_classifier.parameters()).device
            except StopIteration:
                router_device = self.device
            feature_vec = feature_vec.to(router_device)

            router_logits = wise_editor.router_classifier(feature_vec)
            probability = torch.sigmoid(router_logits).mean().item()

            return probability