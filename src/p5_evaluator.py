"""
P5æ¨èç³»ç»Ÿå®˜æ–¹è¯„ä¼°å™¨
åŸºäºOpenP5å®˜æ–¹è¯„ä¼°æ–¹æ³•ï¼Œæ”¯æŒæ ‡å‡†çš„æ¨èç³»ç»ŸæŒ‡æ ‡
"""

import numpy as np
import math
import logging
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict

logger = logging.getLogger(__name__)

class P5RecommendationEvaluator:
    """P5æ¨èç³»ç»Ÿè¯„ä¼°å™¨ - åŸºäºå®˜æ–¹è¯„ä¼°æ–¹æ³•"""
    
    def __init__(self):
        self.metrics_history = []
        
    def evaluate_recommendations(self, 
                               predictions: List[Dict],
                               ground_truth: List[Dict],
                               metrics: List[str] = ['hit@5', 'hit@10', 'hit@20', 'ndcg@5', 'ndcg@10', 'ndcg@20'],
                               filtered: bool = True) -> Dict[str, float]:
        """
        è¯„ä¼°æ¨èæ€§èƒ½
        
        Args:
            predictions: é¢„æµ‹ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«user_idå’Œrecommended_items
            ground_truth: çœŸå®æ ‡ç­¾åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«user_idå’Œitem_id
            metrics: è¯„ä¼°æŒ‡æ ‡åˆ—è¡¨
            filtered: æ˜¯å¦è¿‡æ»¤è®­ç»ƒé›†ä¸­çš„ç‰©å“
        
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        logger.info(f"å¼€å§‹P5æ¨èç³»ç»Ÿè¯„ä¼°ï¼ŒæŒ‡æ ‡: {metrics}")
        
        # æ„å»ºç”¨æˆ·-ç‰©å“æ˜ å°„
        user_predictions = self._build_user_predictions(predictions)
        user_ground_truth = self._build_user_ground_truth(ground_truth)
        
        # è®¡ç®—ç›¸å…³æ€§ç»“æœï¼ˆç”¨äº hit å’Œ ndcgï¼‰
        rel_results = self._compute_relevance_results(
            user_predictions, user_ground_truth, max([self._extract_k(m) for m in metrics])
        )
        
        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        results = {}
        for metric in metrics:
            if metric.lower().startswith('hit'):
                k = self._extract_k(metric)
                results[f'hit@{k}'] = self._hit_at_k(rel_results, k)
            elif metric.lower().startswith('ndcg'):
                k = self._extract_k(metric)
                results[f'ndcg@{k}'] = self._ndcg_at_k(rel_results, k)
            elif metric.lower().startswith('recall'):
                k = self._extract_k(metric)
                # ä¿®æ­£ï¼šåŸºäº ground truth å¤§å°è®¡ç®— Recall@K
                results[f'recall@{k}'] = self._recall_at_k_from_maps(user_predictions, user_ground_truth, k)
        
        # è®¡ç®—ç”¨æˆ·æ•°é‡ç”¨äºæ ‡å‡†åŒ–
        total_users = len(user_ground_truth)
        
        # æ•´ç†ç»“æœï¼ˆæ— éœ€äºŒæ¬¡å½’ä¸€åŒ–ï¼Œå› ä¸ºå„æŒ‡æ ‡è®¡ç®—å‡½æ•°å·²ç»æ­£ç¡®å½’ä¸€åŒ–ï¼‰
        normalized_results = {}
        for metric_name, value in results.items():
            normalized_results[metric_name] = value
        
        # æ·»åŠ HRæŒ‡æ ‡ï¼ˆç­‰åŒäºHit Rateï¼‰
        if 'hit@10' in normalized_results:
            normalized_results['hit_rate@10'] = normalized_results['hit@10']
        if 'hit@20' in normalized_results:
            normalized_results['hit_rate@20'] = normalized_results['hit@20']
        
        logger.info("P5è¯„ä¼°å®Œæˆ")
        self._log_metrics(normalized_results)
        
        return normalized_results
    
    def _build_user_predictions(self, predictions: List[Dict]) -> Dict[str, List[str]]:
        """æ„å»ºç”¨æˆ·é¢„æµ‹æ˜ å°„"""
        user_preds = defaultdict(list)
        
        for pred in predictions:
            user_id = str(pred.get('user_id', ''))
            items = pred.get('recommended_items', [])
            
            # ç¡®ä¿itemsæ˜¯åˆ—è¡¨
            if isinstance(items, str):
                items = [items]
            elif isinstance(items, (int, float)):
                items = [str(items)]
            
            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶å»é‡
            item_strs = []
            for item in items:
                item_str = str(item)
                if item_str not in item_strs:
                    item_strs.append(item_str)
            
            user_preds[user_id].extend(item_strs)
        
        return user_preds
    
    def _build_user_ground_truth(self, ground_truth: List[Dict]) -> Dict[str, List[str]]:
        """æ„å»ºç”¨æˆ·çœŸå®æ ‡ç­¾æ˜ å°„"""
        user_truth = defaultdict(list)
        
        for truth in ground_truth:
            user_id = str(truth.get('user_id', ''))
            item_id = str(truth.get('item_id', ''))
            
            if item_id and item_id not in user_truth[user_id]:
                user_truth[user_id].append(item_id)
        
        return user_truth
    
    def _compute_relevance_results(self, 
                                  user_predictions: Dict[str, List[str]],
                                  user_ground_truth: Dict[str, List[str]],
                                  max_k: int) -> List[List[int]]:
        """è®¡ç®—ç›¸å…³æ€§ç»“æœï¼ˆæŒ‰ç…§P5å®˜æ–¹æ–¹æ³•ï¼‰"""
        rel_results = []
        
        for user_id in user_ground_truth:
            if user_id not in user_predictions:
                # å¦‚æœæ²¡æœ‰é¢„æµ‹ï¼Œå¡«å……0
                rel_results.append([0] * max_k)
                continue
            
            pred_items = user_predictions[user_id][:max_k]
            true_items = set(user_ground_truth[user_id])
            
            # è®¡ç®—æ¯ä¸ªä½ç½®çš„ç›¸å…³æ€§
            user_rel = []
            for item in pred_items:
                if item in true_items:
                    user_rel.append(1)
                else:
                    user_rel.append(0)
            
            # å¡«å……åˆ°max_ké•¿åº¦
            while len(user_rel) < max_k:
                user_rel.append(0)
            
            rel_results.append(user_rel)
        
        return rel_results
    
    def _extract_k(self, metric: str) -> int:
        """ä»æŒ‡æ ‡åç§°ä¸­æå–kå€¼"""
        try:
            return int(metric.split('@')[1])
        except (IndexError, ValueError):
            return 10  # é»˜è®¤å€¼
    
    def _hit_at_k(self, relevance: List[List[int]], k: int) -> float:
        """
        è®¡ç®—Hit@Kï¼ˆæ­£ç¡®çš„å½’ä¸€åŒ–ç‰ˆæœ¬ï¼‰
        Hit@K = æœ‰æ¨èå‘½ä¸­çš„ç”¨æˆ·æ•° / æ€»ç”¨æˆ·æ•°
        """
        if not relevance:
            return 0.0
            
        correct = 0.0
        total_users = len(relevance)
        
        for row in relevance:
            rel = row[:k]
            if sum(rel) > 0:  # å¦‚æœåœ¨å‰Kä¸ªæ¨èä¸­æœ‰å‘½ä¸­
                correct += 1
                
        return correct / total_users if total_users > 0 else 0.0
    
    def _ndcg_at_k(self, relevance: List[List[int]], k: int) -> float:
        """
        è®¡ç®—NDCG@Kï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰
        NDCG@K = DCG@K / IDCG@K
        å…¶ä¸­IDCG@Kæ˜¯ç†æƒ³æƒ…å†µä¸‹çš„DCG@Kï¼ˆå‡è®¾å‰Kä¸ªæ¨èéƒ½æ˜¯æœ€ç›¸å…³çš„ï¼‰
        """
        if not relevance:
            return 0.0
            
        total_ndcg = 0.0
        num_users = 0
        
        for row in relevance:
            # è®¡ç®—DCG@K
            dcg = 0.0
            for i in range(min(k, len(row))):
                if row[i] > 0:  # åªæœ‰ç›¸å…³ç‰©å“æ‰è®¡ç®—
                    dcg += row[i] / math.log(i + 2, 2)
            
            # è®¡ç®—IDCG@Kï¼ˆç†æƒ³æƒ…å†µä¸‹çš„DCG@Kï¼‰
            # å¯¹äºæ¨èç³»ç»Ÿï¼Œé€šå¸¸å‡è®¾æ¯ä¸ªç”¨æˆ·çš„ç›¸å…³ç‰©å“è¯„åˆ†éƒ½æ˜¯1
            # IDCG@K = ç†æƒ³æƒ…å†µä¸‹å‰Kä¸ªä½ç½®éƒ½æ˜¯ç›¸å…³ç‰©å“(è¯„åˆ†=1)çš„DCG
            num_relevant_items = sum(1 for score in row if score > 0)  # ç”¨æˆ·å®é™…çš„ç›¸å…³ç‰©å“æ•°
            ideal_relevant_at_k = min(k, num_relevant_items)  # ç†æƒ³æƒ…å†µä¸‹å‰Kä¸ªä½ç½®ä¸­çš„ç›¸å…³ç‰©å“æ•°
            
            idcg = 0.0
            for i in range(ideal_relevant_at_k):
                idcg += 1.0 / math.log(i + 2, 2)  # å‡è®¾ç†æƒ³æƒ…å†µä¸‹ç›¸å…³ç‰©å“è¯„åˆ†ä¸º1
            
            # è®¡ç®—NDCG = DCG / IDCG
            if idcg > 0:
                total_ndcg += dcg / idcg
                num_users += 1
        
        return total_ndcg / num_users if num_users > 0 else 0.0
    
    def _recall_at_k_from_maps(self,
                               user_predictions: Dict[str, List[str]],
                               user_ground_truth: Dict[str, List[str]],
                               k: int) -> float:
        """
        è®¡ç®— Recall@Kï¼šsum_users |Pred@K âˆ© GT| / sum_users |GT|
        è¯´æ˜ï¼šå¿…é¡»ä½¿ç”¨ ground truth çš„çœŸå®æ•°é‡ä½œä¸ºåˆ†æ¯ï¼Œè€Œä¸æ˜¯ç›¸å…³æ€§è¡Œçš„å’Œã€‚
        """
        total_hits = 0
        total_relevant = 0
        for uid, gt_items in user_ground_truth.items():
            gt_set = set(gt_items)
            if not gt_set:
                continue
            preds = user_predictions.get(uid, [])[:k]
            hits = sum(1 for it in preds if it in gt_set)
            total_hits += hits
            total_relevant += len(gt_set)
        if total_relevant == 0:
            return 0.0
        return total_hits / total_relevant
    
    def _log_metrics(self, metrics: Dict[str, float]):
        """è®°å½•è¯„ä¼°æŒ‡æ ‡"""
        logger.info("ğŸ“Š P5è¯„ä¼°æŒ‡æ ‡:")
        for metric_name, value in sorted(metrics.items()):
            logger.info(f"  {metric_name}: {value:.4f}")
    
    def evaluate_unlearning_effectiveness(self, 
                                        before_metrics: Dict[str, float],
                                        after_metrics: Dict[str, float],
                                        target_metrics: List[str] = None) -> Dict[str, Any]:
        """
        è¯„ä¼°é—å¿˜æ•ˆæœ
        
        Args:
            before_metrics: é—å¿˜å‰çš„æŒ‡æ ‡
            after_metrics: é—å¿˜åçš„æŒ‡æ ‡
            target_metrics: ç›®æ ‡æŒ‡æ ‡åˆ—è¡¨
        
        Returns:
            é—å¿˜æ•ˆæœåˆ†æç»“æœ
        """
        if target_metrics is None:
            target_metrics = ['hit@10', 'hit@20', 'ndcg@10', 'ndcg@20', 'recall@10', 'recall@20']
        
        effectiveness = {}
        
        for metric in target_metrics:
            # æ£€æŸ¥æŒ‡æ ‡æ˜¯å¦å­˜åœ¨
            before_value = before_metrics.get(metric, 0.0)
            after_value = after_metrics.get(metric, 0.0)
            
            # è®¡ç®—å˜åŒ–
            absolute_change = after_value - before_value
            relative_change = (absolute_change / before_value * 100) if before_value > 0 else 0.0
            
            effectiveness[metric] = {
                'before': before_value,
                'after': after_value,
                'absolute_change': absolute_change,
                'relative_change': relative_change
            }
        
        return effectiveness
    
    def compute_unlearning_score(self, 
                                retain_effectiveness: Dict[str, Any],
                                unlearn_effectiveness: Dict[str, Any],
                                target_metrics: List[str] = None) -> float:
        """
        è®¡ç®—é—å¿˜æ•ˆæœç»¼åˆåˆ†æ•°
        
        ç†æƒ³æƒ…å†µï¼š
        - ä¿ç•™é›†æ€§èƒ½å˜åŒ–å°½é‡å°ï¼ˆæ¥è¿‘0ï¼‰
        - é—å¿˜é›†æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼ˆè´Ÿå€¼ï¼‰
        
        åˆ†æ•°è®¡ç®—ï¼šé—å¿˜é›†æ€§èƒ½ä¸‹é™ç¨‹åº¦ - ä¿ç•™é›†æ€§èƒ½å˜åŒ–ç¨‹åº¦
        """
        if target_metrics is None:
            target_metrics = ['hit@10', 'hit@20', 'ndcg@10', 'ndcg@20']
        
        total_score = 0.0
        valid_metrics = 0
        
        for metric in target_metrics:
            if metric in retain_effectiveness and metric in unlearn_effectiveness:
                # ä¿ç•™é›†ç›¸å¯¹å˜åŒ–çš„ç»å¯¹å€¼ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
                retain_change = abs(retain_effectiveness[metric]['relative_change'])
                
                # é—å¿˜é›†ç›¸å¯¹å˜åŒ–çš„ç»å¯¹å€¼ï¼ˆè¶Šå¤§è¶Šå¥½ï¼Œå› ä¸ºæœŸæœ›æ€§èƒ½ä¸‹é™ï¼‰
                unlearn_change = abs(unlearn_effectiveness[metric]['relative_change'])
                
                # è®¡ç®—å•ä¸ªæŒ‡æ ‡çš„é—å¿˜æ•ˆæœåˆ†æ•°
                # å¦‚æœé—å¿˜é›†æ€§èƒ½ä¸‹é™ä¸”ä¿ç•™é›†æ€§èƒ½ä¿æŒç¨³å®šï¼Œåˆ™åˆ†æ•°ä¸ºæ­£
                metric_score = unlearn_change - retain_change
                total_score += metric_score
                valid_metrics += 1
        
        return total_score / valid_metrics if valid_metrics > 0 else 0.0
    
    def generate_comparison_report(self, 
                                    before_retain: Dict[str, float],
                                    after_retain: Dict[str, float],
                                    before_unlearn: Dict[str, float],
                                    after_unlearn: Dict[str, float]) -> str:
            """[CIU å‡çº§ç‰ˆ] ç”Ÿæˆè¯¦ç»†çš„äº¤äº’çº§é—å¿˜å¯¹æ¯”æŠ¥å‘Š"""
            
            report_lines = []
            report_lines.append("=" * 110)
            report_lines.append("P5æ¨¡å‹ [äº¤äº’çº§é—å¿˜] æ•ˆæœè¯¦ç»†æŠ¥å‘Š (CIU Scheme)")
            report_lines.append("=" * 110)
            report_lines.append("è¯´æ˜: 'ä¿ç•™æ•ˆç”¨'è¡¡é‡å¯¹ç”¨æˆ·å…¶ä»–åå¥½çš„æ¨èèƒ½åŠ›(è¶Šé«˜è¶Šå¥½)ï¼Œ'é—å¿˜æ•ˆèƒ½'è¡¡é‡å¯¹ç‰¹å®šç‰©å“çš„é—å¿˜ç¨‹åº¦(è¶Šä½è¶Šå¥½)ã€‚")
            
            # è¡¨å¤´
            header = f"{'æŒ‡æ ‡':^25} | {'é—å¿˜å‰':^15} | {'é—å¿˜å':^15} | {'å˜åŒ–(%)':^12} || {'é—å¿˜å‰':^15} | {'é—å¿˜å':^15} | {'å˜åŒ–(%)':^12}"
            subheader = f"{' ':^25} | {'--- ä¿ç•™é›† (ä¿ç•™æ•ˆç”¨) ---':^46} || {'--- é—å¿˜é›† (ä¿ç•™æ•ˆç”¨) ---':^46}"
            report_lines.append(subheader)
            report_lines.append(header)
            report_lines.append("-" * 110)
            
            # å…³é”®æŒ‡æ ‡
            key_metrics_retain = ['hit@10_retain', 'ndcg@10_retain', 'recall@10_retain']
            
            for metric in key_metrics_retain:
                # ä¿ç•™é›†
                br, ar = before_retain.get(metric, 0), after_retain.get(metric, 0)
                retain_change = ((ar - br) / br * 100) if br > 1e-6 else 0
                
                # é—å¿˜é›†
                bu, au = before_unlearn.get(metric, 0), after_unlearn.get(metric, 0)
                unlearn_change_retain = ((au - bu) / bu * 100) if bu > 1e-6 else 0
                
                report_lines.append(
                    f"{metric:^25} | {br:^15.4f} | {ar:^15.4f} | {retain_change:^12.1f} || {bu:^15.4f} | {au:^15.4f} | {unlearn_change_retain:^12.1f}"
                )
            
            report_lines.append("-" * 110)
            
            # é—å¿˜æ•ˆèƒ½çš„å•ç‹¬æŠ¥å‘Š
            header_forget = f"{'æŒ‡æ ‡':^25} | {'é—å¿˜å‰ (å¬å›ç‡)':^20} | {'é—å¿˜å (å¬å›ç‡)':^20} | {'ä¸‹é™ç‡ (%)':^15}"
            subheader_forget = f"{' ':^25} | {'--- é—å¿˜é›† (é—å¿˜æ•ˆèƒ½) ---':^60}"
            report_lines.append("\n" + subheader_forget)
            report_lines.append(header_forget)
            report_lines.append("-" * 85)
            
            key_metrics_forget = ['recall@10_forgotten', 'recall@20_forgotten']
            avg_forget_reduction = []

            for metric in key_metrics_forget:
                if metric in before_unlearn and metric in after_unlearn:
                    bu_f, au_f = before_unlearn.get(metric, 0), after_unlearn.get(metric, 0)
                    forget_reduction = ((bu_f - au_f) / bu_f * 100) if bu_f > 1e-6 else 0
                    avg_forget_reduction.append(forget_reduction)
                    report_lines.append(f"{metric:^25} | {bu_f:^20.4f} | {au_f:^20.4f} | {forget_reduction:^15.1f}")
            
            report_lines.append("-" * 85)
            
            # è®¡ç®—ç»¼åˆè¯„ä¼°
            retain_changes = [abs(((after_retain.get(m,0)-before_retain.get(m,0))/before_retain.get(m,1e-6)*100)) for m in key_metrics_retain if m in before_retain]
            forget_utility_changes = [abs(((after_unlearn.get(m,0)-before_unlearn.get(m,0))/before_unlearn.get(m,1e-6)*100)) for m in key_metrics_retain if m in before_unlearn]
            
            avg_retain_perf_change = np.mean(retain_changes) if retain_changes else 0
            avg_forget_utility_change = np.mean(forget_utility_changes) if forget_utility_changes else 0
            avg_forget_efficacy = np.mean(avg_forget_reduction) if avg_forget_reduction else 0

            report_lines.append(f"\nğŸ¯ é—å¿˜æ•ˆæœç»¼åˆè¯„ä¼°:")
            report_lines.append(f"  - ä¿ç•™é›†æ€§èƒ½ç¨³å®šæ€§: å¹³å‡å˜åŒ– {avg_retain_perf_change:.2f}% (è¶Šå°è¶Šå¥½)")
            report_lines.append(f"  - é—å¿˜é›†é€šç”¨æ€§èƒ½å½±å“: å¹³å‡å˜åŒ– {avg_forget_utility_change:.2f}% (è¶Šå°è¶Šå¥½)")
            report_lines.append(f"  - é—å¿˜é›†ç‰¹å®šé¡¹é—å¿˜ç‡: å¹³å‡ä¸‹é™ {avg_forget_efficacy:.2f}% (è¶Šå¤§è¶Šå¥½)")
            
            score = avg_forget_efficacy - (avg_retain_perf_change + avg_forget_utility_change) / 2
            report_lines.append(f"  - ç»¼åˆåˆ†æ•° (é—å¿˜ç‡ - å¹³å‡æ€§èƒ½å½±å“): {score:.2f}")

            if score > 50:
                report_lines.append("\nâœ… é—å¿˜æ•ˆæœä¼˜ç§€ï¼æ¨¡å‹ç²¾å‡†åœ°é—å¿˜äº†ç‰¹å®šäº¤äº’ï¼ŒåŒæ—¶å‡ ä¹æ²¡æœ‰å½±å“å…¶ä»–æ¨èçš„è´¨é‡ã€‚")
            elif score > 20:
                report_lines.append("\nğŸ‘ é—å¿˜æ•ˆæœè‰¯å¥½ï¼Œä½†å¯¹é€šç”¨æ¨èæ€§èƒ½æœ‰è½»å¾®å½±å“ã€‚")
            else:
                report_lines.append("\nâš ï¸ é—å¿˜æ•ˆæœä¸€èˆ¬æˆ–ä¸ä½³ï¼Œè¯·æ£€æŸ¥è¶…å‚æ•°æˆ–è®­ç»ƒè½®æ•°ã€‚")

            report_lines.append("=" * 110)
            
            return "\n".join(report_lines)
