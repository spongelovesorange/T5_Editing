#!/usr/bin/env python3
"""
[æœ€ç»ˆä¸€ä½“åŒ–ç‰ˆ]
Entry point for dual-memory fine-tuning.
é›†æˆäº†å¯è¯„ä¼°æ•°æ®é›†çš„è‡ªåŠ¨ç”ŸæˆåŠŸèƒ½ã€‚
"""
import os
os.environ['HF_HUB_OFFLINE'] = '1'

import argparse
import logging
from pathlib import Path
import json
from collections import defaultdict
import random
import pandas as pd
import torch
from typing import Optional, Dict, Any
from src.dual_memory import DualMemoryConfig, train_dual_memory

# --- æ•°æ®æ–‡ä»¶è·¯å¾„é…ç½® ---
PROJECT_ROOT = Path(__file__).resolve().parent
DEFAULT_DATA_DIR = PROJECT_ROOT / "data" / "ML1M"
INTER_FILE = DEFAULT_DATA_DIR / "ml-1m.inter"
USER_INDEX_FILE = DEFAULT_DATA_DIR / "user_indexing.txt"
OUTPUT_FORGET_FILE = PROJECT_ROOT / 'results' / 'forget_samples_subset.json'
OUTPUT_RETAIN_FILE = PROJECT_ROOT / 'results' / 'retain_samples_subset.json'
# --- é…ç½®ç»“æŸ ---

def prepare_guaranteed_datasets(
    num_forget_samples: int = 500,
    num_retain_samples: int = 565, # [æ ¸å¿ƒä¿®æ”¹1] ä¸ºä¿ç•™é›†ä¹Ÿè®¾ç½®æ•°é‡
    forget_percentage: float = 0.05, # [æ ¸å¿ƒä¿®æ”¹1] é—å¿˜äº¤äº’çš„ç™¾åˆ†æ¯”
    force_regenerate: bool = False
):
    """
    [æœ€ç»ˆå…¬å¹³è¯„ä¼°ç‰ˆ]
    è‡ªåŠ¨ç”Ÿæˆæˆ–éªŒè¯é—å¿˜/ä¿ç•™æ•°æ®é›†ã€‚
    1. ç­›é€‰å‡ºä¸€ä¸ª"é»„é‡‘å€™é€‰æ± "ï¼Œç¡®ä¿è¿™äº›ç”¨æˆ·çš„è¡Œä¸ºåœ¨æµ‹è¯•é›†ä¸­æ˜¯å¯è§çš„ã€‚
    2. ä»åŒä¸€ä¸ªæ± å­ä¸­ä¸ºé—å¿˜é›†å’Œä¿ç•™é›†æŠ½æ ·ï¼Œä¿è¯è¯„ä¼°åŸºçº¿çš„å…¬å¹³æ€§ã€‚
    3. ä¸ºé—å¿˜ç”¨æˆ·é€‰å–å…¶æœ€è¿‘5%çš„äº¤äº’ä½œä¸ºé—å¿˜ç›®æ ‡ã€‚
    """
    logging.info("--- [æ­¥éª¤ 1/3] å‡†å¤‡å¯è¯„ä¼°çš„æ•°æ®é›† (å…¬å¹³è¯„ä¼°ç‰ˆ) ---")
    
    # æå‰åŠ è½½å¹¶åˆ†å‰²æ•°æ®ï¼Œå› ä¸ºæ— è®ºå¦‚ä½•éƒ½éœ€è¦ test_df
    logging.info("â³ æ­£åœ¨åŠ è½½å’Œåˆ†å‰²äº¤äº’æ•°æ®...")
    df = pd.read_csv(INTER_FILE, sep='\t', header=None, names=['user_id', 'item_id', 'rating', 'timestamp'], dtype={'user_id': str, 'item_id': str})
    user_map = {line.strip().split()[0]: line.strip().split()[1] for line in open(USER_INDEX_FILE)}
    df['mapped_user_id'] = df['user_id'].map(user_map)
    df.dropna(subset=['mapped_user_id'], inplace=True)
    df = df.sort_values('timestamp')
    split_point = int(len(df) * 0.8)
    train_df = df.iloc[:split_point]
    test_df = df.iloc[split_point:]
    logging.info("âœ… äº¤äº’æ•°æ®åŠ è½½åˆ†å‰²å®Œæˆã€‚")

    if OUTPUT_FORGET_FILE.exists() and OUTPUT_RETAIN_FILE.exists() and not force_regenerate:
        logging.info(f"âœ… æ•°æ®é›† '{OUTPUT_FORGET_FILE.name}' å’Œ '{OUTPUT_RETAIN_FILE.name}' å·²å­˜åœ¨ï¼Œè·³è¿‡ç”Ÿæˆã€‚")
        return test_df # ç›´æ¥è¿”å›åŠ è½½å¥½çš„ test_df

    logging.info("â³ å¼€å§‹æ„é€ ä¿è¯è¯„ä¼°å…¬å¹³çš„é—å¿˜/ä¿ç•™æ•°æ®é›†...")

    # 1. åŠ è½½IDæ˜ å°„
    rev_user_map = {v: k for k, v in user_map.items()}
    
    # 2. [æ ¸å¿ƒä¿®æ”¹2] æ„é€ â€œé»„é‡‘å€™é€‰æ± â€
    logging.info("ğŸ” æ­£åœ¨ç­›é€‰è¡Œä¸ºå¯é¢„æµ‹çš„ 'é»„é‡‘å€™é€‰æ± ' ç”¨æˆ·...")
    train_user_history_counts = train_df['mapped_user_id'].value_counts()
    # ç­›é€‰å‡ºåœ¨è®­ç»ƒé›†ä¸­æœ‰è¶³å¤Ÿå†å²ï¼ˆä¾‹å¦‚è¶…è¿‡10æ¬¡ï¼‰çš„ç”¨æˆ·
    candidate_users_from_train = set(train_user_history_counts[train_user_history_counts >= 10].index)
    # ç­›é€‰å‡ºåœ¨æµ‹è¯•é›†ä¸­æœ‰è‡³å°‘ä¸€æ¬¡é«˜åˆ†äº¤äº’çš„ç”¨æˆ·
    test_set_active_users = set(test_df[test_df['rating'] >= 4]['mapped_user_id'].unique())
    
    golden_pool = sorted(list(candidate_users_from_train.intersection(test_set_active_users)))
    random.shuffle(golden_pool)
    logging.info(f"âœ… 'é»„é‡‘å€™é€‰æ± ' æ„é€ å®Œæˆï¼Œå…± {len(golden_pool)} åå€™é€‰ç”¨æˆ·ã€‚")

    if len(golden_pool) < num_forget_samples + num_retain_samples:
        raise RuntimeError(
            f"é»„é‡‘å€™é€‰æ± ç”¨æˆ·ä¸è¶³ ({len(golden_pool)})ï¼Œæ— æ³•æ»¡è¶³é—å¿˜é›†({num_forget_samples})å’Œä¿ç•™é›†({num_retain_samples})çš„æ•°é‡éœ€æ±‚ã€‚"
        )

    # 3. ä»åŒä¸€ä¸ªæ± å­ä¸­æŠ½æ ·ï¼Œä¿è¯å…¬å¹³æ€§
    forget_user_ids = set(golden_pool[:num_forget_samples])
    retain_user_ids = set(golden_pool[num_forget_samples : num_forget_samples + num_retain_samples])
    
    logging.info(f"ğŸ‘¥ å·²ä»æ± ä¸­æŠ½æ ·: {len(forget_user_ids)} åé—å¿˜ç”¨æˆ·, {len(retain_user_ids)} åä¿ç•™ç”¨æˆ·ã€‚")

    # 4. [æ ¸å¿ƒä¿®æ”¹3] ç”Ÿæˆé—å¿˜æ ·æœ¬ï¼Œé—å¿˜æœ€è¿‘5%çš„äº¤äº’
    forget_samples = []
    for user_id in forget_user_ids:
        user_history_df = train_df[train_df['mapped_user_id'] == user_id].sort_values('timestamp')
        history_items = user_history_df['item_id'].tolist()
        
        if len(history_items) < 5: continue # å†å²å¤ªçŸ­çš„ç”¨æˆ·è·³è¿‡

        num_to_forget = max(1, int(len(history_items) * forget_percentage))
        
        items_to_forget = history_items[-num_to_forget:]
        history_for_prompt = history_items[:-num_to_forget]

        if not history_for_prompt: continue

        forget_sample = {
            "user_id": rev_user_map.get(user_id, ""),
            "seq_items": history_for_prompt,
            "suppression_targets": items_to_forget
        }
        forget_samples.append(forget_sample)

    if not forget_samples:
        raise RuntimeError("æœªèƒ½ç”Ÿæˆä»»ä½•æœ‰æ•ˆçš„é—å¿˜æ ·æœ¬ï¼Œè¯·æ£€æŸ¥æ•°æ®å’Œé€»è¾‘ã€‚")

    logging.info(f"ğŸ’¾ æˆåŠŸç”Ÿæˆ {len(forget_samples)} æ¡é—å¿˜æ ·æœ¬ (æ¯æ¡é—å¿˜ {forget_percentage*100:.0f}% äº¤äº’)ï¼Œæ­£åœ¨ä¿å­˜...")
    with open(OUTPUT_FORGET_FILE, 'w') as f:
        json.dump(forget_samples, f, indent=2)

    # 5. ç”Ÿæˆä¿ç•™é›†æ ·æœ¬ (ä»–ä»¬ä¹Ÿæ¥è‡ªé»„é‡‘æ± )
    retain_samples = []
    for user_id in retain_user_ids:
        user_history = train_df[train_df['mapped_user_id'] == user_id]['item_id'].tolist()
        if len(user_history) > 1:
            retain_samples.append({
                "user_id": rev_user_map.get(user_id, ""),
                "seq_items": user_history
            })
    
    logging.info(f"ğŸ’¾ æˆåŠŸç”Ÿæˆ {len(retain_samples)} æ¡ä¿ç•™æ ·æœ¬ï¼Œæ­£åœ¨ä¿å­˜...")
    with open(OUTPUT_RETAIN_FILE, 'w') as f:
        json.dump(retain_samples, f, indent=2)
    logging.info(f"âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆã€‚")
    
    return test_df

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Train dual-memory adapter and router")
    parser.add_argument("--model", required=True, help="Path to base P5 checkpoint (.pt)")
    parser.add_argument(
        "--forget",
        default=str(OUTPUT_FORGET_FILE),
        help="Path to forget samples JSON",
    )
    parser.add_argument(
        "--retain",
        default=str(OUTPUT_RETAIN_FILE),
        help="Path to retain samples JSON",
    )
    parser.add_argument(
        "--output",
        default="results/dual_memory",
        help="Directory to store artifacts",
    )
    parser.add_argument("--device", default=None, help="Device string, e.g. cuda or cpu")
    parser.add_argument("--seed", type=int, default=2025, help="Random seed")
    parser.add_argument("--batch-size", type=int, default=8, help="Dual-memory batch size")
    parser.add_argument("--epochs", type=int, default=5, help="Dual-memory epochs")
    parser.add_argument("--lr", type=float, default=5e-4, help="Dual-memory learning rate")
    parser.add_argument("--workers", type=int, default=8, help="DataLoader worker processes (default: 8)")
    parser.add_argument("--amp", action="store_true", help="Enable mixed precision (AMP) training")
    parser.add_argument(
        "--beta-weight", type=float, default=3.0, help="Weight for the forgetting loss term."
    )
    parser.add_argument(
        "--alpha-weight", type=float, default=1.0, help="Weight for the retention loss term."
    )
    parser.add_argument(
        "--kl-reg-weight", type=float, default=10.0, help="Weight for KL-divergence regularization."
    )
    parser.add_argument("--gradient-clip", type=float, default=1.0, help="Max norm for gradient clipping")

    parser.add_argument("--forget-ratio", type=float, default=1.0, help="Subset ratio for forget set")
    parser.add_argument("--retain-ratio", type=float, default=1.0, help="Subset ratio for retain set")
    parser.add_argument("--max-input-length", type=int, default=256, help="Prompt token limit")
    parser.add_argument("--max-target-length", type=int, default=32, help="Target token limit")
    parser.add_argument("--lora-r", type=int, default=8, help="LoRA rank")
    parser.add_argument("--lora-alpha", type=float, default=16.0, help="LoRA alpha")
    parser.add_argument("--lora-dropout", type=float, default=0.0, help="LoRA dropout")
    parser.add_argument("--router-hidden", type=int, default=64, help="Router hidden dim")
    parser.add_argument("--router-dropout", type=float, default=0.1, help="Router dropout")
    parser.add_argument("--router-lr", type=float, default=1e-3, help="Router learning rate")
    parser.add_argument("--router-weight-decay", type=float, default=1e-4, help="Router weight decay")
    parser.add_argument("--router-epochs", type=int, default=40, help="Router training epochs")
    parser.add_argument("--router-batch-size", type=int, default=64, help="Router batch size")
    parser.add_argument(
        "--router-target-precision",
        type=float,
        default=0.8,
        help="Desired precision threshold when deriving router cutoff",
    )
    parser.add_argument(
        "--kl-weight",
        type=float,
        default=0.1,
        help="Weight for KL alignment loss on the batch (default: 0.1)",
    )
    # === æ–°å¢ï¼šæ›´å¼ºå¿˜è®°ä¸ç¨³å®šæ€§æ§åˆ¶å‚æ•° ===
    parser.add_argument(
        "--unlikelihood-weight",
        type=float,
        default=0.0,
        help="Weight for unlikelihood loss on forgotten targets (default: 0.0 to disable)",
    )
    parser.add_argument(
        "--pairwise-weight",
        type=float,
        default=0.0,
        help="Weight for pairwise margin ranking loss (default: 0.0 to disable)",
    )
    parser.add_argument(
        "--pairwise-margin",
        type=float,
        default=1.0,
        help="Margin m in pairwise loss max(0, m + s_forgot - s_neg) (default: 1.0)",
    )
    parser.add_argument(
        "--hard-neg-k",
        type=int,
        default=50,
        help="Top-K hard negatives from main model logits for pairwise loss (default: 50)",
    )
    parser.add_argument(
        "--kl-mask-forgotten",
        action="store_true",
        help="If set, do not apply KL regularization on forgotten samples (mask them out).",
    )
    parser.add_argument(
        "--freeze-lm-head",
        action="store_true",
        help="Freeze lm_head (no LoRA/grad) to reduce collateral drift.",
    )
    parser.add_argument(
        "--topk-penalty-weight",
        type=float,
        default=0.0,
        help="Weight for TopK containment penalty to suppress forgotten target within side top-K (default: 0.0)",
    )
    parser.add_argument(
        "--topk-k",
        type=int,
        default=20,
        help="K for TopK containment penalty (default: 20)",
    )
    parser.add_argument(
        "--topk-margin",
        type=float,
        default=0.0,
        help="Margin for TopK containment penalty (default: 0.0)",
    )
    # ç»å¯¹é˜ˆå€¼å‹åˆ¶ï¼ˆå°†è¢«é—å¿˜ç›®æ ‡çš„logitå‹åˆ°å›ºå®šè´Ÿé˜ˆå€¼ä»¥ä¸‹ï¼‰
    parser.add_argument(
        "--abs-suppression-weight",
        type=float,
        default=0.0,
        help="Weight for absolute suppression penalty on forgotten target logits (default: 0.0 to disable)",
    )
    parser.add_argument(
        "--abs-suppression-margin",
        type=float,
        default=3.0,
        help="Margin m for absolute suppression: penalize max(0, m + logit)^2 so that logit <= -m (default: 3.0)",
    )
    parser.add_argument(
        "--edit-layer",
        action="append",
        default=None,
        help="Layer to instrument with LoRA (repeatable)",
    )
    parser.add_argument(
        "--router-layer",
        action="append",
        default=None,
        help="Layer to capture for router features (repeatable)",
    )
    parser.add_argument(
        "--init-artifacts",
        default=None,
        help="Existing dual-memory artifacts for warm start",
    )
    parser.add_argument(
        "--activation-margin",
        type=float,
        default=5.0,
        help="Margin for activation difference between side/main on forget samples",
    )
    parser.add_argument(
        "--force-regenerate-data",
        action="store_true",
        help="å¦‚æœæŒ‡å®šï¼Œå°†å¼ºåˆ¶é‡æ–°ç”Ÿæˆé—å¿˜/ä¿ç•™æ•°æ®é›†ï¼Œè¦†ç›–ç°æœ‰æ–‡ä»¶ã€‚"
    )
    # é¢„è®¾é…ç½®ï¼Œä¾¿äºä¸€é”®å¤ç°å®éªŒæ–¹æ¡ˆ
    parser.add_argument(
        "--preset",
        type=str,
        choices=["e1_stable", "e2_strong"],
        default=None,
        help="å¯é€‰è¶…å‚é¢„è®¾: e1_stable(ç¨³æ€ä¼˜å…ˆ) | e2_strong(æ›´å¼ºé—å¿˜)",
    )
    return parser.parse_args()

def main() -> None:
    args = parse_args()
    logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")

    # [é›†æˆåŠŸèƒ½] åœ¨è®­ç»ƒå‰ï¼Œè‡ªåŠ¨è°ƒç”¨æ•°æ®å‡†å¤‡å‡½æ•°
    test_df = prepare_guaranteed_datasets(force_regenerate=args.force_regenerate_data)

    # æ ¹æ®é¢„è®¾è¦†ç›–å…³é”®è¶…å‚ï¼ˆå‘½ä»¤è¡Œæ˜¾å¼ä¼ å…¥çš„å‚æ•°ä»å¯è¦†ç›–è¿™äº›å€¼ï¼‰
    if args.preset == "e1_stable":
        logging.info("âš™ï¸ ä½¿ç”¨é¢„è®¾: e1_stable (ç¨³æ€ä¼˜å…ˆ)")
        # ç¨³å®šä¿ç•™æ•ˆç”¨ï¼ŒæŠ‘åˆ¶ NDCG æ¼ç½‘é«˜ä½
        if args.pairwise_weight == 0.0: args.pairwise_weight = 0.6
        if args.pairwise_margin == 1.0: args.pairwise_margin = 1.5
        if args.kl_reg_weight == 10.0: args.kl_reg_weight = 15.0
        if args.alpha_weight == 1.0: args.alpha_weight = 40.0
        if args.beta_weight == 3.0: args.beta_weight = 120.0
        if args.unlikelihood_weight == 0.0: args.unlikelihood_weight = 1.0
        if args.hard_neg_k == 50: args.hard_neg_k = 50
        # å¼ºåˆ¶æ‰“å¼€ä¸¤é¡¹ç¨³æ€å¼€å…³
        args.kl_mask_forgotten = True
        args.freeze_lm_head = True
    elif args.preset == "e2_strong":
        logging.info("âš™ï¸ ä½¿ç”¨é¢„è®¾: e2_strong (æ›´å¼ºé—å¿˜)")
        # æ›´å¼ºå‹åˆ¶æ¼ç½‘é¡¹ï¼Œå…è®¸æ›´å¤§å¹…åº¦çš„é—å¿˜
        if args.pairwise_weight == 0.0: args.pairwise_weight = 1.2
        if args.pairwise_margin == 1.0: args.pairwise_margin = 2.0
        if args.kl_reg_weight == 10.0: args.kl_reg_weight = 12.0
        if args.alpha_weight == 1.0: args.alpha_weight = 35.0
        if args.beta_weight == 3.0: args.beta_weight = 150.0
        if args.unlikelihood_weight == 0.0: args.unlikelihood_weight = 1.5
        if args.hard_neg_k == 50: args.hard_neg_k = 50
        # å¼€å¯ç»å¯¹é˜ˆå€¼å‹åˆ¶ï¼ˆå¯è¦†ç›–ï¼‰
        if args.abs_suppression_weight == 0.0: args.abs_suppression_weight = 1.5
        if args.abs_suppression_margin == 3.0: args.abs_suppression_margin = 3.0
        args.kl_mask_forgotten = True
        args.freeze_lm_head = True

    logging.info("--- [æ­¥éª¤ 2/3] å¼€å§‹åŒè®°å¿†æ¨¡å‹è®­ç»ƒ ---")
    config = DualMemoryConfig(
        model_path=args.model,
        forget_path=args.forget,
        retain_path=str(OUTPUT_RETAIN_FILE),
        output_dir=args.output,
        device=args.device if args.device else "cuda" if torch.cuda.is_available() else "cpu",
        seed=args.seed,
        batch_size=args.batch_size,
        epochs=args.epochs,
        lr=args.lr,
        gradient_clip=args.gradient_clip,
        forget_ratio=args.forget_ratio,
        retain_ratio=args.retain_ratio,
        max_input_length=args.max_input_length,
        max_target_length=args.max_target_length,
        lora_r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        # å…³é”®ä¿®å¤ï¼šå°† CLI çš„ alpha/beta æ­£ç¡®ä¼ é€’ç»™è®­ç»ƒå™¨ä½¿ç”¨çš„å­—æ®µ
        alpha_weight=args.alpha_weight,
        beta_weight=args.beta_weight,
    dataloader_num_workers=args.workers,
    use_amp=bool(args.amp),
        router_hidden_dim=args.router_hidden,
        router_dropout=args.router_dropout,
        router_lr=args.router_lr,
        router_weight_decay=args.router_weight_decay,
        router_epochs=args.router_epochs,
        router_batch_size=args.router_batch_size,
        router_target_precision=args.router_target_precision,
        retain_kl_weight=args.alpha_weight, # ä»…ç”¨äºå…¼å®¹æ€§ä¿ç•™ï¼ˆè®­ç»ƒå™¨ä¸è¯»å–è¯¥å­—æ®µï¼‰
        kl_reg_weight=args.kl_reg_weight,
        activation_separation_weight=0.0,
        forget_suppression_weight=args.beta_weight, # ä»…ç”¨äºå…¼å®¹æ€§ä¿ç•™ï¼ˆè®­ç»ƒå™¨ä¸è¯»å–è¯¥å­—æ®µï¼‰
        # æ–°å¢å­—æ®µä¼ é€’
        unlikelihood_weight=args.unlikelihood_weight,
        pairwise_weight=args.pairwise_weight,
        pairwise_margin=args.pairwise_margin,
        hard_neg_k=args.hard_neg_k,
        kl_mask_forgotten=args.kl_mask_forgotten,
        freeze_lm_head=args.freeze_lm_head,
        topk_penalty_weight=args.topk_penalty_weight,
        topk_k=args.topk_k,
        topk_margin=args.topk_margin,
        abs_suppression_weight=args.abs_suppression_weight,
        abs_suppression_margin=args.abs_suppression_margin,
    )

    if args.edit_layer: config.edit_target_layers = args.edit_layer
    if args.router_layer: config.router_target_layers = args.router_layer

    logging.info("è®­ç»ƒé…ç½®: %s", json.dumps(config.to_dict(), indent=2))

    init_state: Optional[Dict[str, Any]] = None
    if args.init_artifacts:
        init_path = Path(args.init_artifacts)
        if not init_path.exists():
            raise FileNotFoundError(f"Init artifacts not found: {init_path}")
        logging.info("Loading initial artifacts from %s", init_path)
        init_state = torch.load(init_path, map_location="cpu")

    artifacts = train_dual_memory(config, initial_state=init_state, test_df_for_sampling=test_df)
    logging.info("--- [æ­¥éª¤ 3/3] åŒè®°å¿†æ¨¡å‹è®­ç»ƒå®Œæˆ ---")
    logging.info("âœ… è®­ç»ƒäº§ç‰©å·²ä¿å­˜ã€‚è·¯ç”±å™¨æŒ‡æ ‡: %s", artifacts.router_metrics)

if __name__ == "__main__":
    main()